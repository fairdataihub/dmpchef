{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb52b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nahid\\dmpchef\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ STEP 1 ready\n",
      "ROOT_DIR   : c:\\Users\\Nahid\\dmpchef\n",
      "DATA_PDFS  : c:\\Users\\Nahid\\dmpchef\\data\\NIH_95\n",
      "INDEX_DIR  : c:\\Users\\Nahid\\dmpchef\\data\\faiss_index\n",
      "EXCEL_PATH : c:\\Users\\Nahid\\dmpchef\\data\\inputs\\inputs.xlsx\n",
      "TEMPLATE_MD: c:\\Users\\Nahid\\dmpchef\\data\\inputs\\dmp-template.md\n",
      "OUTPUT_MD  : c:\\Users\\Nahid\\dmpchef\\data\\outputs7\\markdown\n",
      "OUTPUT_DOCX: c:\\Users\\Nahid\\dmpchef\\data\\outputs7\\docx\n",
      "EMBED_MODEL: sentence-transformers/all-MiniLM-L6-v2 | LLM_MODEL: llama3.3 | TOP_K: 6\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 1 ‚Äî Imports, Config, and Helpers\n",
    "# ============================================\n",
    "import os, re, time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import pypandoc  # for Markdown ‚Üí DOCX\n",
    "\n",
    "# --- LangChain Core ---\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ---------- Paths (works in notebook or script) ----------\n",
    "try:\n",
    "    ROOT_DIR = Path(__file__).resolve().parents[1]  # when running a .py script\n",
    "except NameError:\n",
    "    ROOT_DIR = Path.cwd().parent                     # when running inside Jupyter\n",
    "\n",
    "# --- Data folders ---\n",
    "#DATA_PDFS   = ROOT_DIR / \"data\" / \"data_ingestion\" / \"NIH_all\"\n",
    "INDEX_DIR   = ROOT_DIR / \"data\" / \"vector_db\"/\"NIH_all_db\"\n",
    "EXCEL_PATH  = ROOT_DIR / \"data\" / \"inputs\" / \"inputs.xlsx\"\n",
    "TEMPLATE_MD = ROOT_DIR / \"data\" / \"inputs\" / \"dmp-template.md\"\n",
    "\n",
    "# --- Output folders ---\n",
    "OUTPUT_MD   = ROOT_DIR / \"data\" / \"outputs7\" / \"markdown\"\n",
    "OUTPUT_DOCX = ROOT_DIR / \"data\" / \"outputs7\" / \"docx\"\n",
    "\n",
    "# --- Models / parameters ---\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LLM_MODEL   = \"llama3.3\"\n",
    "TOP_K       = 6\n",
    "\n",
    "# ---------- Helper functions ----------\n",
    "def create_folder(folderpath):\n",
    "    Path(folderpath).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_md(folderpath, filename, text):\n",
    "    create_folder(folderpath)\n",
    "    (Path(folderpath) / filename).write_text(text, encoding=\"utf-8\")\n",
    "    print(\"üíæ Saved:\", Path(folderpath) / filename)\n",
    "\n",
    "def md_to_docs(md_filepath, docx_folderpath, docx_filename):\n",
    "    create_folder(docx_folderpath)\n",
    "    pypandoc.convert_file(\n",
    "        str(md_filepath), \"docx\",\n",
    "        outputfile=str(Path(docx_folderpath) / docx_filename)\n",
    "    )\n",
    "    print(\"üìÑ Converted:\", Path(docx_folderpath) / docx_filename)\n",
    "\n",
    "def clean_filename(name: str) -> str:\n",
    "    \"\"\"Remove illegal characters from filenames (Windows-safe).\"\"\"\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", str(name)).strip()\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Sanity print ----------\n",
    "print(\" STEP 1 ready\")\n",
    "print(f\"ROOT_DIR   : {ROOT_DIR}\")\n",
    "print(f\"INDEX_DIR  : {INDEX_DIR}\")\n",
    "print(f\"EXCEL_PATH : {EXCEL_PATH}\")\n",
    "print(f\"TEMPLATE_MD: {TEMPLATE_MD}\")\n",
    "print(f\"OUTPUT_MD  : {OUTPUT_MD}\")\n",
    "print(f\"OUTPUT_DOCX: {OUTPUT_DOCX}\")\n",
    "print(f\"EMBED_MODEL: {EMBED_MODEL} | LLM_MODEL: {LLM_MODEL} | TOP_K: {TOP_K}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1339d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 1 ‚Äî Imports, Config (YAML), and Helpers\n",
    "# ============================================\n",
    "import os, re, time\n",
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import pypandoc  # for Markdown ‚Üí DOCX\n",
    "\n",
    "# --- LangChain Core ---\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ---------- Resolve project root (works in notebook or script) ----------\n",
    "try:\n",
    "    PROJECT_ROOT = Path(__file__).resolve().parents[1]  # when running a .py script\n",
    "except NameError:\n",
    "    PROJECT_ROOT = Path.cwd().parent                    # when running inside Jupyter\n",
    "\n",
    "\n",
    "# ---------- YAML loader + path resolver ----------\n",
    "def load_yaml_config(cfg_path: Path) -> dict:\n",
    "    cfg_path = Path(cfg_path)\n",
    "    if not cfg_path.exists():\n",
    "        raise FileNotFoundError(f\"Config YAML not found: {cfg_path}\")\n",
    "    with cfg_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    if not isinstance(cfg, dict):\n",
    "        raise ValueError(f\"Config YAML must parse to a dict. Got: {type(cfg)}\")\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def resolve_from_root(project_root: Path, root_dir_value: str | Path) -> Path:\n",
    "    \"\"\"\n",
    "    YAML root_dir:\n",
    "      - \".\" means project root\n",
    "      - relative paths are relative to project root\n",
    "      - absolute paths are used as-is\n",
    "    \"\"\"\n",
    "    p = Path(root_dir_value).expanduser()\n",
    "    if p.is_absolute():\n",
    "        return p.resolve()\n",
    "    return (project_root / p).resolve()\n",
    "\n",
    "\n",
    "def resolve_path(base: Path, rel_or_abs: str | Path | None) -> Path | None:\n",
    "    \"\"\"Resolve a path relative to `base` if not absolute. Keep None as None.\"\"\"\n",
    "    if rel_or_abs is None:\n",
    "        return None\n",
    "    p = Path(rel_or_abs).expanduser()\n",
    "    if p.is_absolute():\n",
    "        return p.resolve()\n",
    "    return (base / p).resolve()\n",
    "\n",
    "\n",
    "# ---------- Choose your YAML file here ----------\n",
    "CONFIG_YAML = PROJECT_ROOT / \"config\" / \"config.yaml\"  # change if your file name differs\n",
    "cfg = load_yaml_config(CONFIG_YAML)\n",
    "\n",
    "# ---------- Root dir from YAML ----------\n",
    "ROOT_DIR = resolve_from_root(PROJECT_ROOT, cfg[\"root_dir\"])\n",
    "\n",
    "# ---------- Paths from YAML (direct access) ----------\n",
    "DATA_PDFS   = resolve_path(ROOT_DIR, cfg[\"paths\"][\"data_pdfs\"])        # optional in your pipeline, but required here\n",
    "INDEX_DIR   = resolve_path(ROOT_DIR, cfg[\"paths\"][\"index_dir\"])\n",
    "EXCEL_PATH  = resolve_path(ROOT_DIR, cfg[\"paths\"][\"excel_path\"])\n",
    "OUTPUT_MD   = resolve_path(ROOT_DIR, cfg[\"paths\"][\"output_md\"])\n",
    "OUTPUT_DOCX = resolve_path(ROOT_DIR, cfg[\"paths\"][\"output_docx\"])\n",
    "\n",
    "# If you have template_md in YAML, use it. Otherwise keep your existing default file.\n",
    "TEMPLATE_MD = resolve_path(\n",
    "    ROOT_DIR,\n",
    "    cfg[\"paths\"].get(\"template_md\", \"data/inputs/dmp-template.md\")\n",
    ")\n",
    "\n",
    "# ---------- RAG params from YAML (direct access) ----------\n",
    "TOP_K = int(cfg[\"rag\"][\"retriever_top_k\"])\n",
    "\n",
    "# ---------- Models from YAML (direct access) ----------\n",
    "EMBED_MODEL = cfg[\"models\"][\"embedding_model\"]\n",
    "LLM_MODEL   = cfg[\"models\"][\"llm_name\"]\n",
    "\n",
    "EMBED_DEVICE       = cfg[\"models\"][\"embedding_device\"]\n",
    "EMBED_BATCH_SIZE   = int(cfg[\"models\"][\"embedding_batch_size\"])\n",
    "NORMALIZE_EMBEDS   = bool(cfg[\"models\"][\"normalize_embeddings\"])\n",
    "HF_CACHE_DIR       = resolve_path(ROOT_DIR, cfg[\"models\"][\"hf_cache_dir\"])\n",
    "LOCAL_FILES_ONLY   = bool(cfg[\"models\"][\"local_files_only\"])\n",
    "ALLOW_DL_IF_MISS   = bool(cfg[\"models\"][\"allow_download_if_missing\"])\n",
    "\n",
    "\n",
    "# ---------- Helper functions ----------\n",
    "def create_folder(folderpath: Path | str) -> None:\n",
    "    Path(folderpath).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_md(folderpath: Path | str, filename: str, text: str) -> Path:\n",
    "    create_folder(folderpath)\n",
    "    out_path = Path(folderpath) / filename\n",
    "    out_path.write_text(text, encoding=\"utf-8\")\n",
    "    print(\"Saved:\", out_path)\n",
    "    return out_path\n",
    "\n",
    "def md_to_docs(md_filepath: Path | str, docx_folderpath: Path | str, docx_filename: str) -> Path:\n",
    "    create_folder(docx_folderpath)\n",
    "    out_path = Path(docx_folderpath) / docx_filename\n",
    "    pypandoc.convert_file(str(md_filepath), \"docx\", outputfile=str(out_path))\n",
    "    print(\"Converted:\", out_path)\n",
    "    return out_path\n",
    "\n",
    "def clean_filename(name: str) -> str:\n",
    "    \"\"\"Remove illegal characters from filenames (Windows-safe).\"\"\"\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", str(name)).strip()\n",
    "\n",
    "\n",
    "# ---------- Sanity print ----------\n",
    "print(\"STEP 1 ready (YAML-driven, direct access)\")\n",
    "print(f\"CONFIG_YAML : {CONFIG_YAML}\")\n",
    "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")\n",
    "print(f\"ROOT_DIR    : {ROOT_DIR}\")\n",
    "print(f\"DATA_PDFS   : {DATA_PDFS}\")\n",
    "print(f\"INDEX_DIR   : {INDEX_DIR}\")\n",
    "print(f\"EXCEL_PATH  : {EXCEL_PATH}\")\n",
    "print(f\"TEMPLATE_MD : {TEMPLATE_MD}\")\n",
    "print(f\"OUTPUT_MD   : {OUTPUT_MD}\")\n",
    "print(f\"OUTPUT_DOCX : {OUTPUT_DOCX}\")\n",
    "print(f\"EMBED_MODEL : {EMBED_MODEL}\")\n",
    "print(f\"LLM_MODEL   : {LLM_MODEL}\")\n",
    "print(f\"TOP_K       : {TOP_K}\")\n",
    "print(f\"EMBED_DEVICE: {EMBED_DEVICE} | BATCH: {EMBED_BATCH_SIZE} | NORMALIZE: {NORMALIZE_EMBEDS}\")\n",
    "print(f\"HF_CACHE_DIR: {HF_CACHE_DIR} | local_files_only={LOCAL_FILES_ONLY} | allow_download_if_missing={ALLOW_DL_IF_MISS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "687935b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üì• Loading PDFs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:22<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 586 pages from 105 PDFs.\n",
      "‚úÖ Created 2016 chunks from 586 pages.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 2 ‚Äî Load PDFs and Split into Text Chunks\n",
    "# ============================================\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def load_pdfs_from_folder(folder: Path):\n",
    "    \"\"\"Load all PDF files from a folder into LangChain Document objects.\"\"\"\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"‚ùå Folder not found: {folder}\")\n",
    "    pdf_files = sorted(folder.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(f\"‚ö†Ô∏è No PDF files found in {folder}\")\n",
    "\n",
    "    docs = []\n",
    "    for pdf_path in tqdm(pdf_files, desc=\"üì• Loading PDFs\"):\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_path))\n",
    "            docs.extend(loader.load())\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipped {pdf_path.name}: {e}\")\n",
    "\n",
    "    print(f\"‚úÖ Loaded {len(docs)} pages from {len(pdf_files)} PDFs.\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "def split_into_chunks(docs, chunk_size=800, chunk_overlap=120):\n",
    "    \"\"\"Split PDF text into overlapping chunks for embedding/indexing.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    print(f\"‚úÖ Created {len(chunks)} chunks from {len(docs)} pages.\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# --- Run quick test ---\n",
    "raw_docs = load_pdfs_from_folder(DATA_PDFS)\n",
    "chunks = split_into_chunks(raw_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e494d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ sentence-transformers version: 5.2.2\n"
     ]
    }
   ],
   "source": [
    "import sentence_transformers\n",
    "print(\"‚úÖ sentence-transformers version:\", sentence_transformers.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a0f00cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nahid\\AppData\\Local\\Temp\\ipykernel_45832\\2831991515.py:10: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 1537.31it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Existing FAISS index found. Loading from disk...\n",
      "‚úÖ FAISS index loaded successfully.\n",
      "‚úÖ Retriever ready (top_k=6)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 3 ‚Äî Build or Load FAISS Index\n",
    "# ============================================\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "import time\n",
    "\n",
    "# --- Initialize embedding model ---\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "def build_or_load_faiss_index(index_dir=INDEX_DIR, chunks=None):\n",
    "    \"\"\"\n",
    "    Builds a new FAISS index from text chunks if none exists,\n",
    "    otherwise loads the saved one from disk.\n",
    "    \"\"\"\n",
    "    faiss_path = index_dir / \"index.faiss\"\n",
    "    pkl_path   = index_dir / \"index.pkl\"\n",
    "\n",
    "    # --- If index exists, load it ---\n",
    "    if faiss_path.exists() and pkl_path.exists():\n",
    "        print(\"üì¶ Existing FAISS index found. Loading from disk...\")\n",
    "        vectorstore = FAISS.load_local(\n",
    "            str(index_dir),\n",
    "            embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        print(\"‚úÖ FAISS index loaded successfully.\")\n",
    "        return vectorstore\n",
    "\n",
    "    # --- Otherwise, build new index ---\n",
    "    if chunks is None or len(chunks) == 0:\n",
    "        raise RuntimeError(\"‚ùå No chunks provided. Please run Step 2 first to load and split PDFs.\")\n",
    "\n",
    "    print(\"üß± Building new FAISS index...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        tqdm(chunks, desc=\"üî¢ Embedding text chunks\"),\n",
    "        embeddings\n",
    "    )\n",
    "\n",
    "    # --- Save the index ---\n",
    "    vectorstore.save_local(str(index_dir))\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    print(f\"üíæ Saved new FAISS index to {index_dir}\")\n",
    "    print(f\"‚è±Ô∏è Build completed in {duration/60:.2f} minutes ({duration:.1f} seconds)\")\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "# --- Execute step ---\n",
    "vectorstore = build_or_load_faiss_index(INDEX_DIR, chunks)\n",
    "retriever   = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "print(f\"‚úÖ Retriever ready (top_k={TOP_K})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1d9b2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Excel loaded successfully: 26 rows\n",
      "‚úÖ DMP Markdown template loaded.\n",
      "üîó RAG chain initialized with model: llama3.3 | few-shot examples: 3\n",
      "‚úÖ RAG chain ready for generation.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# üß© STEP 4 ‚Äî Load Excel, Template, and Build RAG Chain (Few-shot from Excel)\n",
    "# ============================================\n",
    "import re\n",
    "import pandas as pd\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# --- Load Excel file ---\n",
    "if not EXCEL_PATH.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Excel file not found: {EXCEL_PATH}\")\n",
    "\n",
    "df = pd.read_excel(EXCEL_PATH)\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "df = df.fillna(\"\")\n",
    "print(f\"‚úÖ Excel loaded successfully: {len(df)} rows\")\n",
    "\n",
    "# --- Load Markdown Template ---\n",
    "if not TEMPLATE_MD.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Template file not found: {TEMPLATE_MD}\")\n",
    "\n",
    "dmp_template_text = TEMPLATE_MD.read_text(encoding=\"utf-8\")\n",
    "print(\"‚úÖ DMP Markdown template loaded.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ‚úÖ Few-shot builder from Excel (Element-aware + relevant)\n",
    "# ============================================================\n",
    "def _infer_element_from_question(q: str) -> str:\n",
    "    m = re.search(r\"element\\s*([1-6])\", q, flags=re.IGNORECASE)\n",
    "    return m.group(1) if m else \"1\"\n",
    "\n",
    "def _row_to_fewshot_example(row: pd.Series, element_num: str) -> str:\n",
    "    title = str(row.get(\"title\", \"\")).strip()\n",
    "    designation = str(row.get(\"designation\", \"\")).strip()\n",
    "\n",
    "    if element_num == \"1\":\n",
    "        a = str(row.get(\"element_1a\", \"\")).strip()\n",
    "        b = str(row.get(\"element_1b\", \"\")).strip()\n",
    "        c = str(row.get(\"element_1c\", \"\")).strip()\n",
    "        answer = (\n",
    "            \"**Element 1: Data Type**\\n\\n\"\n",
    "            \"1. **Types and amount of scientific data expected to be generated in the project:**\\n\"\n",
    "            f\"{a}\\n\\n\"\n",
    "            \"2. **Scientific data that will be preserved and shared, and the rationale for doing so:**\\n\"\n",
    "            f\"{b}\\n\\n\"\n",
    "            \"3. **Metadata, other relevant data, and associated documentation:**\\n\"\n",
    "            f\"{c}\"\n",
    "        )\n",
    "\n",
    "    elif element_num == \"2\":\n",
    "        answer = \"**Element 2: Related Tools, Software and/or Code**\\n\\n\" + str(row.get(\"element_2\", \"\")).strip()\n",
    "\n",
    "    elif element_num == \"3\":\n",
    "        answer = \"**Element 3: Standards**\\n\\n\" + str(row.get(\"element_3\", \"\")).strip()\n",
    "\n",
    "    elif element_num == \"4\":\n",
    "        a = str(row.get(\"element_4a\", \"\")).strip()\n",
    "        b = str(row.get(\"element_4b\", \"\")).strip()\n",
    "        c = str(row.get(\"element_4c\", \"\")).strip()\n",
    "        answer = (\n",
    "            \"**Element 4: Data Preservation, Access, and Associated Timelines**\\n\\n\"\n",
    "            \"1. **Repository and preservation timeline:**\\n\"\n",
    "            f\"{a}\\n\\n\"\n",
    "            \"2. **How data will be discoverable/findable:**\\n\"\n",
    "            f\"{b}\\n\\n\"\n",
    "            \"3. **Access, sharing mechanisms, and timelines:**\\n\"\n",
    "            f\"{c}\"\n",
    "        )\n",
    "\n",
    "    elif element_num == \"5\":\n",
    "        a = str(row.get(\"element_5a\", \"\")).strip()\n",
    "        b = str(row.get(\"element_5b\", \"\")).strip()\n",
    "        c = str(row.get(\"element_5c\", \"\")).strip()\n",
    "        answer = (\n",
    "            \"**Element 5: Access, Distribution, or Reuse Considerations**\\n\\n\"\n",
    "            \"1. **Factors affecting access/sharing:**\\n\"\n",
    "            f\"{a}\\n\\n\"\n",
    "            \"2. **Steps for access / distribution:**\\n\"\n",
    "            f\"{b}\\n\\n\"\n",
    "            \"3. **Privacy / confidentiality protections:**\\n\"\n",
    "            f\"{c}\"\n",
    "        )\n",
    "\n",
    "    else:  # element_num == \"6\"\n",
    "        answer = \"**Element 6: Oversight of Data Management and Sharing**\\n\\n\" + str(row.get(\"element_6\", \"\")).strip()\n",
    "\n",
    "    q = f'Write Element {element_num} for a project similar to: \"{title}\" ({designation}).'\n",
    "    return f\"### Example\\nQuestion:\\n{q}\\n\\nAnswer:\\n{answer}\"\n",
    "\n",
    "\n",
    "def build_few_shot_block_from_excel(question: str, n_examples: int = 3) -> str:\n",
    "    element_num = _infer_element_from_question(question)\n",
    "\n",
    "    needed_cols = {\n",
    "        \"1\": [\"element_1a\", \"element_1b\", \"element_1c\"],\n",
    "        \"2\": [\"element_2\"],\n",
    "        \"3\": [\"element_3\"],\n",
    "        \"4\": [\"element_4a\", \"element_4b\", \"element_4c\"],\n",
    "        \"5\": [\"element_5a\", \"element_5b\", \"element_5c\"],\n",
    "        \"6\": [\"element_6\"],\n",
    "    }[element_num]\n",
    "\n",
    "    # keep only rows that have content for that element\n",
    "    df_valid = df.copy()\n",
    "    for c in needed_cols:\n",
    "        if c not in df_valid.columns:\n",
    "            # column missing in file ‚Üí return no few-shot instead of crashing\n",
    "            return \"\"\n",
    "    df_valid = df_valid[(df_valid[needed_cols].astype(str).apply(lambda s: s.str.strip().ne(\"\")).all(axis=1))]\n",
    "\n",
    "    if df_valid.empty:\n",
    "        return \"\"\n",
    "\n",
    "    # simple relevance score: overlap between question words and row metadata fields\n",
    "    q_terms = set(re.findall(r\"[a-zA-Z]{3,}\", question.lower()))\n",
    "\n",
    "    def score_row(r):\n",
    "        hay = f\"{r.get('title','')} {r.get('designation','')} {r.get('institute','')} {r.get('consentdescription','')}\"\n",
    "        hay_terms = set(re.findall(r\"[a-zA-Z]{3,}\", str(hay).lower()))\n",
    "        return len(q_terms & hay_terms)\n",
    "\n",
    "    df_valid = df_valid.copy()\n",
    "    df_valid[\"__score\"] = df_valid.apply(score_row, axis=1)\n",
    "    df_top = df_valid.sort_values(\"__score\", ascending=False).head(n_examples)\n",
    "\n",
    "    examples = [_row_to_fewshot_example(row, element_num) for _, row in df_top.iterrows()]\n",
    "    return \"\\n\\n---\\n\\n\".join(examples)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# --- Build RAG chain (Few-shot + RAG grounding)\n",
    "# ============================================\n",
    "def build_rag_chain(retriever, llm_model=LLM_MODEL, n_few_shot: int = 3):\n",
    "    llm = Ollama(model=llm_model)\n",
    "    parser = StrOutputParser()\n",
    "\n",
    "    def format_docs(docs):\n",
    "        if not docs:\n",
    "            return \"\"\n",
    "        formatted = []\n",
    "        for d in docs:\n",
    "            page = d.metadata.get(\"page\", \"\")\n",
    "            title = d.metadata.get(\"source\", \"\")\n",
    "            formatted.append(f\"[Page {page}] {title}\\n{d.page_content.strip()}\")\n",
    "        return \"\\n\\n\".join(formatted)\n",
    "\n",
    "    def make_few_shot(q: str) -> str:\n",
    "        return build_few_shot_block_from_excel(q, n_examples=n_few_shot)\n",
    "\n",
    "    prompt_template = \"\"\"You are an expert biomedical data steward and grant writer.\n",
    "Create a high-quality NIH Data Management and Sharing Plan (DMSP) based on the retrieved NIH context and the user's query.\n",
    "\n",
    "You MUST follow the formatting and style demonstrated by the few-shot examples.\n",
    "\n",
    "---- Few-shot examples (from your Excel) ----\n",
    "{few_shot}\n",
    "\n",
    "---- Context from NIH Repository (grounding) ----\n",
    "{context}\n",
    "\n",
    "---- Question ----\n",
    "{question}\n",
    "\n",
    "Rules:\n",
    "- Use NIH context when relevant; do NOT invent policy details.\n",
    "- If a specific policy detail is not supported by the provided context, write: \"Not specified in provided NIH context.\"\n",
    "- Follow the NIH template structure and keep section titles unchanged when the template is provided.\n",
    "\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(template=prompt_template, input_variables=[\"few_shot\", \"context\", \"question\"])\n",
    "\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": retriever | format_docs,\n",
    "            \"few_shot\": RunnablePassthrough() | make_few_shot,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | parser\n",
    "    )\n",
    "\n",
    "    print(f\"üîó RAG chain initialized with model: {llm_model} | few-shot examples: {n_few_shot}\")\n",
    "    return rag_chain\n",
    "\n",
    "\n",
    "# --- Initialize the RAG chain ---\n",
    "rag_chain = build_rag_chain(retriever, n_few_shot=3)\n",
    "print(\"‚úÖ RAG chain ready for generation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd1758e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded input Excel ‚Äî 26 rows\n",
      "‚úÖ Loaded NIH DMP Markdown template from: c:\\Users\\Nahid\\dmpchef\\data\\inputs\\dmp-template.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:   0%|          | 0/26 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß© Generating DMP for: Clinical and MRI data from human research participants\n",
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Clinical and MRI data from human research participants.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:   4%|‚ñç         | 1/26 [01:16<31:46, 76.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Clinical and MRI data from human research participants.docx\n",
      "\n",
      "üß© Generating DMP for: Genomic data from human research participants\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:   8%|‚ñä         | 2/26 [02:11<25:29, 63.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Genomic data from human research participants.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Genomic data from human research participants.docx\n",
      "\n",
      "üß© Generating DMP for: Genomic data from a non-human source\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  12%|‚ñà‚ñè        | 3/26 [03:04<22:35, 58.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Genomic data from a non-human source.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Genomic data from a non-human source.docx\n",
      "\n",
      "üß© Generating DMP for: Secondary data analysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  15%|‚ñà‚ñå        | 4/26 [04:05<21:57, 59.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Secondary data analysis.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Secondary data analysis.docx\n",
      "\n",
      "üß© Generating DMP for: Human clinical and genomics data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  19%|‚ñà‚ñâ        | 5/26 [05:07<21:09, 60.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Human clinical and genomics data.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Human clinical and genomics data.docx\n",
      "\n",
      "üß© Generating DMP for: Gene expression analysis data from non-human model organism (zebrafish)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  23%|‚ñà‚ñà‚ñé       | 6/26 [06:08<20:15, 60.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Gene expression analysis data from non-human model organism (zebrafish).md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Gene expression analysis data from non-human model organism (zebrafish).docx\n",
      "\n",
      "üß© Generating DMP for: Human survey data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  27%|‚ñà‚ñà‚ñã       | 7/26 [07:02<18:32, 58.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Human survey data.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Human survey data.docx\n",
      "\n",
      "üß© Generating DMP for: Clinical Data from Human Research Participants\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  31%|‚ñà‚ñà‚ñà       | 8/26 [08:03<17:48, 59.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Clinical Data from Human Research Participants.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Clinical Data from Human Research Participants.docx\n",
      "\n",
      "üß© Generating DMP for: Human genomic data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [09:09<17:22, 61.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Human genomic data.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Human genomic data.docx\n",
      "\n",
      "üß© Generating DMP for: Technology development\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [10:20<17:08, 64.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Technology development.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Technology development.docx\n",
      "\n",
      "üß© Generating DMP for: Basic Research from a Non-Human Source Example\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 11/26 [11:18<15:34, 62.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Basic Research from a Non-Human Source Example.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Basic Research from a Non-Human Source Example.docx\n",
      "\n",
      "üß© Generating DMP for: Secondary Data Analysis Example\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [12:15<14:10, 60.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Secondary Data Analysis Example.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Secondary Data Analysis Example.docx\n",
      "\n",
      "üß© Generating DMP for: Survey and Interview Example\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 13/26 [13:08<12:41, 58.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Survey and Interview Example.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Survey and Interview Example.docx\n",
      "\n",
      "üß© Generating DMP for: Human Clinical Trial Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [14:05<11:36, 58.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Human Clinical Trial Data.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Human Clinical Trial Data.docx\n",
      "\n",
      "üß© Generating DMP for: Clinical data from human research participants-NIA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [15:04<10:41, 58.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Clinical data from human research participants-NIA.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Clinical data from human research participants-NIA.docx\n",
      "\n",
      "üß© Generating DMP for: Survey, interview, and biological data (tiered access)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [15:59<09:34, 57.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Survey, interview, and biological data (tiered access).md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Survey, interview, and biological data (tiered access).docx\n",
      "\n",
      "üß© Generating DMP for: Non-human data (primates)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [16:50<08:17, 55.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Non-human data (primates).md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Non-human data (primates).docx\n",
      "\n",
      "üß© Generating DMP for: Secondary data analysis-NIA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [17:51<07:36, 57.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Secondary data analysis-NIA.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Secondary data analysis-NIA.docx\n",
      "\n",
      "üß© Generating DMP for: Survey and interview data-NIA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [18:42<06:26, 55.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Survey and interview data-NIA.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Survey and interview data-NIA.docx\n",
      "\n",
      "üß© Generating DMP for: Human clinical and genomic data-NIA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [20:02<06:15, 62.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Human clinical and genomic data-NIA.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Human clinical and genomic data-NIA.docx\n",
      "\n",
      "üß© Generating DMP for: Non-human data (rodents)-NIA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [21:14<05:27, 65.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Non-human data (rodents)-NIA.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Non-human data (rodents)-NIA.docx\n",
      "\n",
      "üß© Generating DMP for: Clinical data (human biospecimens)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [22:15<04:16, 64.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Clinical data (human biospecimens).md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Clinical data (human biospecimens).docx\n",
      "\n",
      "üß© Generating DMP for: Drug discovery including intellectual property\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [23:15<03:08, 62.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Drug discovery including intellectual property.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Drug discovery including intellectual property.docx\n",
      "\n",
      "üß© Generating DMP for: HeLa Cell Whole Genome Sequence (DNA or RNA)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [24:20<02:07, 63.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\HeLa Cell Whole Genome Sequence (DNA or RNA).md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\HeLa Cell Whole Genome Sequence (DNA or RNA).docx\n",
      "\n",
      "üß© Generating DMP for: Secondary Data Analysis on Data from Human Subjects-NIA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [25:20<01:02, 62.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Secondary Data Analysis on Data from Human Subjects-NIA.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Secondary Data Analysis on Data from Human Subjects-NIA.docx\n",
      "\n",
      "üß© Generating DMP for: Analysis of social media posts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [26:17<00:00, 60.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\\Analysis of social media posts.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Analysis of social media posts.docx\n",
      "\n",
      "‚úÖ Finished processing all rows.\n",
      "üìä CSV log saved to: c:\\Users\\Nahid\\dmpchef\\data\\outputs\\rag_generated_dmp_log.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# üß© STEP 5 ‚Äî RAG-Based DMP Generation Using Titles (UPDATED for Few-shot RAG Chain)\n",
    "# ============================================\n",
    "import re, pandas as pd, pypandoc\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- Paths ----------\n",
    "EXCEL_PATH = ROOT_DIR / \"data\" / \"inputs\" / \"inputs.xlsx\"\n",
    "OUTPUT_LOG = ROOT_DIR / \"data\" / \"outputs7\" / \"rag_generated_dmp_log.csv\"\n",
    "OUTPUT_MD.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DOCX.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Load Excel ----------\n",
    "df = pd.read_excel(EXCEL_PATH)\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "df = df.fillna(\"\")\n",
    "print(f\"‚úÖ Loaded input Excel ‚Äî {len(df)} rows\")\n",
    "\n",
    "# ---------- Verify template ----------\n",
    "if not TEMPLATE_MD.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Template not found: {TEMPLATE_MD}\")\n",
    "dmp_template_text = TEMPLATE_MD.read_text(encoding=\"utf-8\")\n",
    "print(f\"‚úÖ Loaded NIH DMP Markdown template from: {TEMPLATE_MD}\")\n",
    "\n",
    "# ---------- Helper functions ----------\n",
    "def sanitize_filename(name: str) -> str:\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", str(name).strip())\n",
    "\n",
    "def create_folder(folderpath: Path):\n",
    "    folderpath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_md(folderpath: Path, filename: str, response: str):\n",
    "    create_folder(folderpath)\n",
    "    filepath = folderpath / filename\n",
    "    filepath.write_text(response, encoding=\"utf-8\")\n",
    "    print(f\"üíæ Saved: {filepath}\")\n",
    "\n",
    "def md_to_docx(md_filepath: Path, docx_folder: Path, docx_filename: str):\n",
    "    create_folder(docx_folder)\n",
    "    docx_path = docx_folder / docx_filename\n",
    "    pypandoc.convert_file(str(md_filepath), \"docx\", outputfile=str(docx_path))\n",
    "    print(f\"üìÑ Converted: {docx_path}\")\n",
    "\n",
    "# ---------- Main Generation ----------\n",
    "records = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"üß† Generating NIH DMPs\"):\n",
    "    title = str(row.get(\"title\", \"\")).strip()\n",
    "    if not title:\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nüß© Generating DMP for: {title}\")\n",
    "\n",
    "    # 1) Build proposal background from the row\n",
    "    element_texts = []\n",
    "    for col in [c for c in df.columns if c.startswith(\"element\")]:\n",
    "        val = str(row.get(col, \"\")).strip()\n",
    "        if val:\n",
    "            element_texts.append(f\"{col.upper()}: {val}\")\n",
    "    query_data = \"\\n\".join(element_texts)\n",
    "\n",
    "    # 2) Build question that rag_chain expects (rag_chain does retrieval + few-shot)\n",
    "    question = f\"\"\"\n",
    "Create a complete NIH Data Management and Sharing Plan (DMSP) for the project titled: \"{title}\".\n",
    "\n",
    "Use the NIH DMSP Markdown template below and DO NOT change section titles.\n",
    "\n",
    "Project background / proposal details:\n",
    "{query_data}\n",
    "\n",
    "NIH DMSP Markdown template:\n",
    "{dmp_template_text}\n",
    "\"\"\".strip()\n",
    "\n",
    "    # 3) Generate\n",
    "    try:\n",
    "        response = rag_chain.invoke(question)\n",
    "\n",
    "        safe_title = sanitize_filename(title)\n",
    "        md_filename = f\"{safe_title}.md\"\n",
    "        docx_filename = f\"{safe_title}.docx\"\n",
    "        md_path = OUTPUT_MD / md_filename\n",
    "\n",
    "        save_md(OUTPUT_MD, md_filename, response)\n",
    "        md_to_docx(md_path, OUTPUT_DOCX, docx_filename)\n",
    "\n",
    "        records.append({\n",
    "            \"Title\": title,\n",
    "            \"Question_Preview\": question[:1000],\n",
    "            \"Generated_DMP_Preview\": response[:1000],\n",
    "            \"Error\": \"\"\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error generating DMP for {title}: {e}\")\n",
    "        records.append({\n",
    "            \"Title\": title,\n",
    "            \"Question_Preview\": question[:1000],\n",
    "            \"Generated_DMP_Preview\": \"\",\n",
    "            \"Error\": str(e)\n",
    "        })\n",
    "\n",
    "# ---------- Save Log ----------\n",
    "pd.DataFrame(records).to_csv(OUTPUT_LOG, index=False, encoding=\"utf-8\")\n",
    "print(\"\\n‚úÖ Finished processing all rows.\")\n",
    "print(f\"üìä CSV log saved to: {OUTPUT_LOG}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db58c3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ ROOT_DIR set to: C:\\Users\\Nahid\\dmpchef\n",
      "üìó Gold PDF folder     : C:\\Users\\Nahid\\dmpchef\\data\\inputs\\gold_dmps   (exists=True)\n",
      "üìò Generated MD folder : C:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown (exists=True)\n",
      "üìä Found 26 generated .md files\n",
      "üìä Found 26 gold .pdf files\n",
      "üöÄ Loading models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 1551.42it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Models ready.\n",
      "üìä Indexed 26 generated DMPs and 26 gold PDFs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Matching & Comparing DMPs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:09<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Results saved to: C:\\Users\\Nahid\\dmpchef\\data\\outputs1\\evaluation_results\\full_dmp_pdf_comparison_fuzzy.csv\n",
      "üßæ Total matched DMP pairs: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# üß© STEP 6 ‚Äî Full DMP Comparison: Markdown (Generated) vs PDF (Gold, Fuzzy Matching)\n",
    "# ============================================\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ‚úÖ Robust ROOT_DIR discovery\n",
    "# ------------------------------------------------------------\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Walk upward until we find a directory that looks like the project root.\n",
    "    Heuristics: contains a 'data' folder and (optionally) 'config' or 'dmpchef'.\n",
    "    \"\"\"\n",
    "    cur = start.resolve()\n",
    "    for _ in range(20):\n",
    "        if (cur / \"data\").exists() and ((cur / \"config\").exists() or (cur / \"dmpchef\").exists()):\n",
    "            return cur\n",
    "        if cur.parent == cur:\n",
    "            break\n",
    "        cur = cur.parent\n",
    "    return start.resolve()\n",
    "\n",
    "ROOT_DIR = find_project_root(Path.cwd())\n",
    "print(f\"üìÇ ROOT_DIR set to: {ROOT_DIR}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ‚úÖ Path auto-detect (try outputs1 then outputs)\n",
    "# ------------------------------------------------------------\n",
    "gold_candidates = [\n",
    "    ROOT_DIR / \"data\" / \"inputs\" / \"gold_dmps\",\n",
    "    ROOT_DIR / \"data\" / \"gold_dmps\",\n",
    "]\n",
    "\n",
    "generated_candidates = [\n",
    "    ROOT_DIR / \"data\" / \"outputs7\" / \"markdown\",\n",
    "    ROOT_DIR / \"data\" / \"outputs7\"  / \"markdown\",\n",
    "]\n",
    "\n",
    "def pick_folder(candidates, pattern):\n",
    "    for p in candidates:\n",
    "        if p.exists() and any(p.glob(pattern)):\n",
    "            return p\n",
    "    # return the first existing one even if empty (for debugging)\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return candidates[0]\n",
    "\n",
    "GOLD_DIR = pick_folder(gold_candidates, \"*.pdf\")\n",
    "GENERATED_DIR = pick_folder(generated_candidates, \"*.md\")\n",
    "\n",
    "EVAL_DIR = ROOT_DIR / \"data\" / \"outputs7\" / \"evaluation_results\"\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìó Gold PDF folder     : {GOLD_DIR}   (exists={GOLD_DIR.exists()})\")\n",
    "print(f\"üìò Generated MD folder : {GENERATED_DIR} (exists={GENERATED_DIR.exists()})\")\n",
    "\n",
    "# Debug counts\n",
    "gold_pdf_count = len(list(GOLD_DIR.glob(\"*.pdf\"))) if GOLD_DIR.exists() else 0\n",
    "gen_md_count   = len(list(GENERATED_DIR.glob(\"*.md\"))) if GENERATED_DIR.exists() else 0\n",
    "print(f\"üìä Found {gen_md_count} generated .md files\")\n",
    "print(f\"üìä Found {gold_pdf_count} gold .pdf files\")\n",
    "\n",
    "if gen_md_count == 0:\n",
    "    raise FileNotFoundError(f\"‚ùå No generated Markdown files found in: {GENERATED_DIR}\")\n",
    "if gold_pdf_count == 0:\n",
    "    raise FileNotFoundError(f\"‚ùå No gold PDF files found in: {GOLD_DIR}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Models\n",
    "# ------------------------------------------------------------\n",
    "print(\"üöÄ Loading models...\")\n",
    "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "print(\"‚úÖ Models ready.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Helper functions\n",
    "# ------------------------------------------------------------\n",
    "def normalize_name(name: str) -> str:\n",
    "    name = name.lower()\n",
    "    name = re.sub(r\"[^a-z0-9\\s]\", \" \", name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name)\n",
    "    return name.strip()\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL)\n",
    "    text = re.sub(r\"#+\\s*\", \"\", text)\n",
    "    text = re.sub(r\"\\*\\*|\\*\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: Path) -> str:\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text(\"text\") + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading {pdf_path.name}: {e}\")\n",
    "    return clean_text(text)\n",
    "\n",
    "def chunk_text(text, size=300):\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i+size]) for i in range(0, len(words), size)]\n",
    "\n",
    "def compare_chunked(gold_text, gen_text, model):\n",
    "    gold_chunks = chunk_text(gold_text)\n",
    "    gen_chunks  = chunk_text(gen_text)\n",
    "\n",
    "    sbert_scores, rouge_scores = [], []\n",
    "    for g in gold_chunks:\n",
    "        emb_g = model.encode(g, convert_to_tensor=True)\n",
    "        sims = []\n",
    "        for gen in gen_chunks:\n",
    "            emb_gen = model.encode(gen, convert_to_tensor=True)\n",
    "            sims.append(util.cos_sim(emb_g, emb_gen).item())\n",
    "        sbert_scores.append(max(sims))\n",
    "\n",
    "        rouge_chunk_scores = [rouge.score(g, gen)[\"rougeL\"].recall for gen in gen_chunks]\n",
    "        rouge_scores.append(max(rouge_chunk_scores))\n",
    "\n",
    "    return float(np.mean(sbert_scores)), float(np.mean(rouge_scores))\n",
    "\n",
    "def best_fuzzy_match(target, gold_names, threshold=0.6):\n",
    "    best_match, best_score = None, 0\n",
    "    for g in gold_names:\n",
    "        score = SequenceMatcher(None, target, g).ratio()\n",
    "        if score > best_score:\n",
    "            best_match, best_score = g, score\n",
    "    return (best_match, best_score) if best_score >= threshold else (None, best_score)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Collect files\n",
    "# ------------------------------------------------------------\n",
    "gold_files = {normalize_name(f.stem): f for f in GOLD_DIR.glob(\"*.pdf\")}\n",
    "gen_files  = {normalize_name(f.stem): f for f in GENERATED_DIR.glob(\"*.md\")}\n",
    "print(f\"üìä Indexed {len(gen_files)} generated DMPs and {len(gold_files)} gold PDFs.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Compare\n",
    "# ------------------------------------------------------------\n",
    "results = []\n",
    "for name, gen_path in tqdm(gen_files.items(), desc=\"üîé Matching & Comparing DMPs\"):\n",
    "    best_match, score = best_fuzzy_match(name, list(gold_files.keys()))\n",
    "    if not best_match:\n",
    "        continue\n",
    "\n",
    "    gold_path = gold_files[best_match]\n",
    "    gold_text = extract_text_from_pdf(gold_path)\n",
    "    gen_text  = clean_text(gen_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "    if not gold_text.strip() or not gen_text.strip():\n",
    "        continue\n",
    "\n",
    "    sbert_sim, rouge_l = compare_chunked(gold_text, gen_text, sbert)\n",
    "    results.append({\n",
    "        \"Generated_File\": gen_path.name,\n",
    "        \"Matched_Gold_PDF\": gold_path.name,\n",
    "        \"Match_Score\": round(score, 3),\n",
    "        \"SBERT_Similarity\": round(sbert_sim, 4),\n",
    "        \"ROUGE_L_Recall\": round(rouge_l, 4),\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "out_path = EVAL_DIR / \"full_dmp_pdf_comparison_fuzzy.csv\"\n",
    "df_results.to_csv(out_path, index=False)\n",
    "print(f\"\\n‚úÖ Results saved to: {out_path}\")\n",
    "print(f\"üßæ Total matched DMP pairs: {len(df_results)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c307ea5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ ROOT_DIR set to: C:\\Users\\Nahid\\dmpchef\n",
      "üìó Gold Excel: C:\\Users\\Nahid\\dmpchef\\data\\inputs\\inputs.xlsx (exists=True)\n",
      "üìò Generated MD folder: C:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown (exists=True)\n",
      "üìÅ Eval output folder : C:\\Users\\Nahid\\dmpchef\\data\\outputs1\\evaluation_results\n",
      "‚úÖ Loaded 26 gold projects.\n",
      "‚úÖ Gold element columns used: ['element_1a', 'element_1b', 'element_1c', 'element_2', 'element_3', 'element_4a', 'element_4b', 'element_4c', 'element_5a', 'element_5b', 'element_5c', 'element_6']\n",
      "üöÄ Loading evaluation models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 1560.06it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Models ready.\n",
      "üîç Found 26 generated Markdown files in: C:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìä Comparing element-level: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:31<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Element-level similarity saved to: C:\\Users\\Nahid\\dmpchef\\data\\outputs1\\evaluation_results\\element_similarity_exact_titles.csv\n",
      "üßæ Total element‚Äìsection best matches: 312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# üß© STEP 7 ‚Äî Element-Level Comparison with NIH Gold Standard (Robust ROOT + folder autodetect)\n",
    "# ============================================\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ‚úÖ Robust ROOT_DIR discovery (do NOT rely on folder name)\n",
    "# ------------------------------------------------------------\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Walk upward until we find a directory that looks like the project root.\n",
    "    Heuristics: contains a 'data' folder and (optionally) 'config' or 'dmpchef'.\n",
    "    \"\"\"\n",
    "    cur = start.resolve()\n",
    "    for _ in range(20):\n",
    "        if (cur / \"data\").exists() and ((cur / \"config\").exists() or (cur / \"dmpchef\").exists()):\n",
    "            return cur\n",
    "        if cur.parent == cur:\n",
    "            break\n",
    "        cur = cur.parent\n",
    "    return start.resolve()\n",
    "\n",
    "ROOT_DIR = find_project_root(Path.cwd())\n",
    "print(f\"üìÇ ROOT_DIR set to: {ROOT_DIR}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ‚úÖ Path auto-detect (try outputs1 then outputs)\n",
    "# ------------------------------------------------------------\n",
    "GOLD_PATH = ROOT_DIR / \"data\" / \"inputs\" / \"inputs.xlsx\"\n",
    "\n",
    "generated_candidates = [\n",
    "    ROOT_DIR / \"data\" / \"outputs7\" / \"markdown\",\n",
    "    ROOT_DIR / \"data\" / \"outputs7\"  / \"markdown\",\n",
    "]\n",
    "\n",
    "eval_candidates = [\n",
    "    ROOT_DIR / \"data\" / \"outputs7\" / \"evaluation_results\",\n",
    "    ROOT_DIR / \"data\" / \"outputs7\"  / \"evaluation_results\",\n",
    "]\n",
    "\n",
    "def pick_existing_folder(candidates):\n",
    "    for p in candidates:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return candidates[0]\n",
    "\n",
    "GENERATED_DIR = pick_existing_folder(generated_candidates)\n",
    "EVAL_DIR = pick_existing_folder(eval_candidates)\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìó Gold Excel: {GOLD_PATH} (exists={GOLD_PATH.exists()})\")\n",
    "print(f\"üìò Generated MD folder: {GENERATED_DIR} (exists={GENERATED_DIR.exists()})\")\n",
    "print(f\"üìÅ Eval output folder : {EVAL_DIR}\")\n",
    "\n",
    "if not GOLD_PATH.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Gold Excel not found: {GOLD_PATH}\")\n",
    "if not GENERATED_DIR.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Generated Markdown folder not found: {GENERATED_DIR}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load gold reference (Excel)\n",
    "# ------------------------------------------------------------\n",
    "df_gold = pd.read_excel(GOLD_PATH)\n",
    "df_gold.columns = df_gold.columns.str.strip().str.lower()\n",
    "df_gold = df_gold.fillna(\"\").astype(str)\n",
    "\n",
    "def normalize_title(name: str) -> str:\n",
    "    name = str(name).lower()\n",
    "    name = re.sub(r\"[^a-z0-9\\s]\", \" \", name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name)\n",
    "    return name.strip()\n",
    "\n",
    "df_gold[\"title_norm\"] = df_gold[\"title\"].apply(normalize_title)\n",
    "\n",
    "gold_elements = [\n",
    "    \"element_1a\",\"element_1b\",\"element_1c\",\n",
    "    \"element_2\",\"element_3\",\n",
    "    \"element_4a\",\"element_4b\",\"element_4c\",\n",
    "    \"element_5a\",\"element_5b\",\"element_5c\",\n",
    "    \"element_6\"\n",
    "]\n",
    "\n",
    "# Keep only element columns that actually exist in the file (avoid KeyErrors)\n",
    "gold_elements = [c for c in gold_elements if c in df_gold.columns]\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df_gold)} gold projects.\")\n",
    "print(f\"‚úÖ Gold element columns used: {gold_elements}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Models\n",
    "# ------------------------------------------------------------\n",
    "print(\"üöÄ Loading evaluation models...\")\n",
    "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "print(\"‚úÖ Models ready.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Markdown parsing helpers\n",
    "# ------------------------------------------------------------\n",
    "def is_title(line: str) -> bool:\n",
    "    s = line.strip()\n",
    "    # Markdown headers (#, ##, ...) OR numbered bold section titles like \"1. **Data Types**\"\n",
    "    return s.startswith(\"#\") or bool(re.match(r\"^\\s*\\d*\\.?\\s*\\*\\*.*\\*\\*\\s*$\", s))\n",
    "\n",
    "def extract_sections(md_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract {Section Title, Generated Content} pairs from a Markdown file.\n",
    "    Also strips any <think>...</think> blocks if present.\n",
    "    \"\"\"\n",
    "    text = md_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL)\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    entries, current_title, buf = [], None, []\n",
    "\n",
    "    for ln in lines:\n",
    "        if is_title(ln):\n",
    "            if current_title and any(x.strip() for x in buf):\n",
    "                entries.append({\n",
    "                    \"Section Title\": current_title.strip(),\n",
    "                    \"Generated Content\": \"\\n\".join(buf).strip()\n",
    "                })\n",
    "            current_title, buf = ln, []\n",
    "        else:\n",
    "            buf.append(ln)\n",
    "\n",
    "    if current_title and any(x.strip() for x in buf):\n",
    "        entries.append({\n",
    "            \"Section Title\": current_title.strip(),\n",
    "            \"Generated Content\": \"\\n\".join(buf).strip()\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(entries)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Compare (exact title match on normalized strings)\n",
    "# ------------------------------------------------------------\n",
    "md_files = sorted(GENERATED_DIR.glob(\"*.md\"))\n",
    "print(f\"üîç Found {len(md_files)} generated Markdown files in: {GENERATED_DIR}\")\n",
    "\n",
    "if len(md_files) == 0:\n",
    "    raise FileNotFoundError(f\"‚ùå No .md files found in {GENERATED_DIR}\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for md_file in tqdm(md_files, desc=\"üìä Comparing element-level\"):\n",
    "    gen_title_raw  = md_file.stem\n",
    "    gen_title_norm = normalize_title(gen_title_raw)\n",
    "\n",
    "    gold_row = df_gold[df_gold[\"title_norm\"] == gen_title_norm]\n",
    "    if gold_row.empty:\n",
    "        # If you want fuzzy matching here too, tell me; for now keep \"exact normalized\"\n",
    "        continue\n",
    "\n",
    "    gold_row = gold_row.iloc[0]\n",
    "    gold_title = gold_row[\"title\"]\n",
    "\n",
    "    # Gather gold element texts\n",
    "    gold_texts = {}\n",
    "    for e in gold_elements:\n",
    "        txt = str(gold_row.get(e, \"\")).strip()\n",
    "        if txt:\n",
    "            gold_texts[e] = txt\n",
    "\n",
    "    if not gold_texts:\n",
    "        continue\n",
    "\n",
    "    # Extract sections from generated MD\n",
    "    gen_df = extract_sections(md_file)\n",
    "    if gen_df.empty:\n",
    "        continue\n",
    "\n",
    "    # Compare each gold element against all generated sections; keep best match by SBERT\n",
    "    for element, gold_text in gold_texts.items():\n",
    "        emb_gold = sbert.encode(gold_text, convert_to_tensor=True)\n",
    "\n",
    "        best = None\n",
    "        best_sbert = -1.0\n",
    "\n",
    "        for _, sec in gen_df.iterrows():\n",
    "            gen_text = str(sec[\"Generated Content\"]).strip()\n",
    "            if not gen_text:\n",
    "                continue\n",
    "\n",
    "            emb_gen = sbert.encode(gen_text, convert_to_tensor=True)\n",
    "            sbert_sim = float(util.cos_sim(emb_gold, emb_gen).item())\n",
    "            rouge_l   = float(rouge.score(gold_text, gen_text)[\"rougeL\"].recall)\n",
    "\n",
    "            if sbert_sim > best_sbert:\n",
    "                best_sbert = sbert_sim\n",
    "                best = {\n",
    "                    \"Gold Project\": gold_title,\n",
    "                    \"Gold Element\": element,\n",
    "                    \"Generated File\": md_file.name,\n",
    "                    \"Generated Section Title\": sec[\"Section Title\"],\n",
    "                    \"SBERT_Similarity\": round(sbert_sim, 4),\n",
    "                    \"ROUGE_L_Recall\": round(rouge_l, 4),\n",
    "                }\n",
    "\n",
    "        if best:\n",
    "            results.append(best)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Save\n",
    "# ------------------------------------------------------------\n",
    "df_results = pd.DataFrame(results)\n",
    "out_path = EVAL_DIR / \"element_similarity_exact_titles.csv\"\n",
    "df_results.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\n‚úÖ Element-level similarity saved to: {out_path}\")\n",
    "print(f\"üßæ Total element‚Äìsection best matches: {len(df_results)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f82ff2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ ROOT_DIR set to: C:\\Users\\Nahid\\dmpchef\n",
      "üìÅ EVAL_DIR: C:\\Users\\Nahid\\dmpchef\\data\\outputs1\\evaluation_results\n",
      "‚úÖ Loaded full-document: 26 rows\n",
      "‚úÖ Loaded element-level: 312 rows\n",
      "\n",
      "üìä Full-document summary table (Mean only, by Generated_File):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Generated_File</th>\n",
       "      <th>SBERT</th>\n",
       "      <th>ROUGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analysis of social media posts.md</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Basic Research from a Non-Human Source Example.md</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clinical Data from Human Research Participants.md</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clinical and MRI data from human research part...</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clinical data (human biospecimens).md</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Clinical data from human research participants...</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Drug discovery including intellectual property.md</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gene expression analysis data from non-human m...</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Genomic data from a non-human source.md</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Genomic data from human research participants.md</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HeLa Cell Whole Genome Sequence (DNA or RNA).md</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Human Clinical Trial Data.md</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Human clinical and genomic data-NIA.md</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Human clinical and genomics data.md</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Human genomic data.md</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Human survey data.md</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Non-human data (primates).md</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Non-human data (rodents)-NIA.md</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Secondary Data Analysis Example.md</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Secondary Data Analysis on Data from Human Sub...</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Secondary data analysis-NIA.md</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Secondary data analysis.md</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Survey and Interview Example.md</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Survey and interview data-NIA.md</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Survey, interview, and biological data (tiered...</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Technology development.md</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Generated_File SBERT ROUGE\n",
       "0                   Analysis of social media posts.md  0.76  0.37\n",
       "1   Basic Research from a Non-Human Source Example.md  0.77  0.35\n",
       "2   Clinical Data from Human Research Participants.md  0.71  0.25\n",
       "3   Clinical and MRI data from human research part...  0.73  0.30\n",
       "4               Clinical data (human biospecimens).md  0.76  0.46\n",
       "5   Clinical data from human research participants...  0.82  0.42\n",
       "6   Drug discovery including intellectual property.md  0.78  0.43\n",
       "7   Gene expression analysis data from non-human m...  0.76  0.48\n",
       "8             Genomic data from a non-human source.md  0.71  0.29\n",
       "9    Genomic data from human research participants.md  0.72  0.30\n",
       "10    HeLa Cell Whole Genome Sequence (DNA or RNA).md  0.86  0.61\n",
       "11                       Human Clinical Trial Data.md  0.63  0.25\n",
       "12             Human clinical and genomic data-NIA.md  0.82  0.57\n",
       "13                Human clinical and genomics data.md  0.65  0.28\n",
       "14                              Human genomic data.md  0.69  0.33\n",
       "15                               Human survey data.md  0.70  0.33\n",
       "16                       Non-human data (primates).md  0.73  0.43\n",
       "17                    Non-human data (rodents)-NIA.md  0.77  0.68\n",
       "18                 Secondary Data Analysis Example.md  0.91  0.76\n",
       "19  Secondary Data Analysis on Data from Human Sub...  0.81  0.51\n",
       "20                     Secondary data analysis-NIA.md  0.79  0.33\n",
       "21                         Secondary data analysis.md  0.65  0.24\n",
       "22                    Survey and Interview Example.md  0.69  0.31\n",
       "23                   Survey and interview data-NIA.md  0.77  0.47\n",
       "24  Survey, interview, and biological data (tiered...  0.81  0.44\n",
       "25                          Technology development.md  0.68  0.69"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Element-level summary table (Mean ¬± SD):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Element</th>\n",
       "      <th>SBERT</th>\n",
       "      <th>ROUGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>element_1a</td>\n",
       "      <td>0.84 ¬± 0.16</td>\n",
       "      <td>0.58 ¬± 0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>element_1b</td>\n",
       "      <td>0.81 ¬± 0.14</td>\n",
       "      <td>0.62 ¬± 0.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>element_1c</td>\n",
       "      <td>0.85 ¬± 0.13</td>\n",
       "      <td>0.65 ¬± 0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>element_2</td>\n",
       "      <td>0.86 ¬± 0.13</td>\n",
       "      <td>0.59 ¬± 0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>element_3</td>\n",
       "      <td>0.83 ¬± 0.13</td>\n",
       "      <td>0.58 ¬± 0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>element_4a</td>\n",
       "      <td>0.86 ¬± 0.08</td>\n",
       "      <td>0.62 ¬± 0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>element_4b</td>\n",
       "      <td>0.89 ¬± 0.11</td>\n",
       "      <td>0.67 ¬± 0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>element_4c</td>\n",
       "      <td>0.90 ¬± 0.08</td>\n",
       "      <td>0.68 ¬± 0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>element_5a</td>\n",
       "      <td>0.80 ¬± 0.18</td>\n",
       "      <td>0.60 ¬± 0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>element_5b</td>\n",
       "      <td>0.85 ¬± 0.13</td>\n",
       "      <td>0.62 ¬± 0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>element_5c</td>\n",
       "      <td>0.81 ¬± 0.17</td>\n",
       "      <td>0.56 ¬± 0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>element_6</td>\n",
       "      <td>0.91 ¬± 0.09</td>\n",
       "      <td>0.72 ¬± 0.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Element        SBERT        ROUGE\n",
       "0   element_1a  0.84 ¬± 0.16  0.58 ¬± 0.39\n",
       "1   element_1b  0.81 ¬± 0.14  0.62 ¬± 0.34\n",
       "2   element_1c  0.85 ¬± 0.13  0.65 ¬± 0.38\n",
       "3    element_2  0.86 ¬± 0.13  0.59 ¬± 0.35\n",
       "4    element_3  0.83 ¬± 0.13  0.58 ¬± 0.29\n",
       "5   element_4a  0.86 ¬± 0.08  0.62 ¬± 0.29\n",
       "6   element_4b  0.89 ¬± 0.11  0.67 ¬± 0.33\n",
       "7   element_4c  0.90 ¬± 0.08  0.68 ¬± 0.29\n",
       "8   element_5a  0.80 ¬± 0.18  0.60 ¬± 0.36\n",
       "9   element_5b  0.85 ¬± 0.13  0.62 ¬± 0.29\n",
       "10  element_5c  0.81 ¬± 0.17  0.56 ¬± 0.39\n",
       "11   element_6  0.91 ¬± 0.09  0.72 ¬± 0.29"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved formatted tables ‚Üí\n",
      "‚Ä¢ C:\\Users\\Nahid\\dmpchef\\data\\outputs1\\evaluation_results\\summary_full_table_mean_only.csv\n",
      "‚Ä¢ C:\\Users\\Nahid\\dmpchef\\data\\outputs1\\evaluation_results\\summary_element_table_mean_sd.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# üßÆ STEP 8 ‚Äî Summarize Evaluation Results (Robust ROOT + outputs1/outputs autodetect)\n",
    "# ============================================\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ‚úÖ Robust ROOT_DIR discovery (do NOT rely on folder name)\n",
    "# ------------------------------------------------------------\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for _ in range(20):\n",
    "        if (cur / \"data\").exists() and ((cur / \"config\").exists() or (cur / \"dmpchef\").exists()):\n",
    "            return cur\n",
    "        if cur.parent == cur:\n",
    "            break\n",
    "        cur = cur.parent\n",
    "    return start.resolve()\n",
    "\n",
    "ROOT_DIR = find_project_root(Path.cwd())\n",
    "print(f\"üìÇ ROOT_DIR set to: {ROOT_DIR}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# ‚úÖ Eval dir autodetect (prefer outputs1 if it has files)\n",
    "# ------------------------------------------------------------\n",
    "eval_candidates = [\n",
    "    ROOT_DIR / \"data\" / \"outputs7\" / \"evaluation_results\",\n",
    "    ROOT_DIR / \"data\" / \"outputs7\"  / \"evaluation_results\",\n",
    "]\n",
    "\n",
    "def pick_eval_dir(cands):\n",
    "    for p in cands:\n",
    "        if p.exists() and any(p.glob(\"*.csv\")):\n",
    "            return p\n",
    "    for p in cands:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return cands[0]\n",
    "\n",
    "EVAL_DIR = pick_eval_dir(eval_candidates)\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"üìÅ EVAL_DIR: {EVAL_DIR}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load CSVs (fail loudly if missing)\n",
    "# ------------------------------------------------------------\n",
    "full_path = EVAL_DIR / \"full_dmp_pdf_comparison_fuzzy.csv\"\n",
    "elem_path = EVAL_DIR / \"element_similarity_exact_titles.csv\"\n",
    "\n",
    "if not full_path.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Missing: {full_path} (Run Step 6 first)\")\n",
    "if not elem_path.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Missing: {elem_path} (Run Step 7 first)\")\n",
    "\n",
    "df_full = pd.read_csv(full_path)\n",
    "df_elem = pd.read_csv(elem_path)\n",
    "\n",
    "print(f\"‚úÖ Loaded full-document: {len(df_full)} rows\")\n",
    "print(f\"‚úÖ Loaded element-level: {len(df_elem)} rows\\n\")\n",
    "\n",
    "if df_full.empty:\n",
    "    raise ValueError(\"‚ùå full_dmp_pdf_comparison_fuzzy.csv is empty (Step 6 matched 0 pairs).\")\n",
    "if df_elem.empty:\n",
    "    raise ValueError(\"‚ùå element_similarity_exact_titles.csv is empty (Step 7 matched 0 pairs).\")\n",
    "\n",
    "# ============================================================\n",
    "# 1) FULL-DOCUMENT LEVEL SUMMARY (Mean only by Generated_File)\n",
    "# ============================================================\n",
    "project_col = \"Generated_File\" if \"Generated_File\" in df_full.columns else df_full.columns[0]\n",
    "\n",
    "# Safer metric detection\n",
    "sbert_col = next((c for c in df_full.columns if \"sbert\" in c.lower()), None)\n",
    "rouge_col = next((c for c in df_full.columns if \"rouge\" in c.lower()), None)\n",
    "\n",
    "if not sbert_col or not rouge_col:\n",
    "    raise ValueError(f\"‚ùå Could not find SBERT/ROUGE columns in: {df_full.columns.tolist()}\")\n",
    "\n",
    "df_full_summary = (\n",
    "    df_full.groupby(project_col)[[sbert_col, rouge_col]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_full_summary[\"SBERT\"] = df_full_summary[sbert_col].map(lambda x: f\"{x:.2f}\")\n",
    "df_full_summary[\"ROUGE\"] = df_full_summary[rouge_col].map(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "df_full_table = df_full_summary[[project_col, \"SBERT\", \"ROUGE\"]].rename(\n",
    "    columns={project_col: \"Generated_File\"}\n",
    ")\n",
    "\n",
    "print(\"üìä Full-document summary table (Mean only, by Generated_File):\")\n",
    "display(df_full_table)\n",
    "\n",
    "# ============================================================\n",
    "# 2) ELEMENT-LEVEL SUMMARY (Mean ¬± SD)\n",
    "# ============================================================\n",
    "elem_col = next((c for c in df_elem.columns if \"element\" in c.lower()), None)\n",
    "if not elem_col:\n",
    "    raise ValueError(f\"‚ùå Could not find element column in: {df_elem.columns.tolist()}\")\n",
    "\n",
    "sbert_col_e = next((c for c in df_elem.columns if \"sbert\" in c.lower()), None)\n",
    "rouge_col_e = next((c for c in df_elem.columns if \"rouge\" in c.lower()), None)\n",
    "\n",
    "if not sbert_col_e or not rouge_col_e:\n",
    "    raise ValueError(f\"‚ùå Could not find SBERT/ROUGE columns in: {df_elem.columns.tolist()}\")\n",
    "\n",
    "df_elem_summary = (\n",
    "    df_elem.groupby(elem_col)[[sbert_col_e, rouge_col_e]]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Flatten columns\n",
    "df_elem_summary.columns = [\n",
    "    elem_col,\n",
    "    \"SBERT_Mean\", \"SBERT_SD\",\n",
    "    \"ROUGE_Mean\", \"ROUGE_SD\"\n",
    "]\n",
    "\n",
    "df_elem_summary[\"SBERT\"] = df_elem_summary.apply(\n",
    "    lambda r: f\"{r['SBERT_Mean']:.2f} ¬± {r['SBERT_SD']:.2f}\", axis=1\n",
    ")\n",
    "df_elem_summary[\"ROUGE\"] = df_elem_summary.apply(\n",
    "    lambda r: f\"{r['ROUGE_Mean']:.2f} ¬± {r['ROUGE_SD']:.2f}\", axis=1\n",
    ")\n",
    "\n",
    "df_elem_table = df_elem_summary[[elem_col, \"SBERT\", \"ROUGE\"]].rename(columns={elem_col: \"Element\"})\n",
    "\n",
    "print(\"\\nüìä Element-level summary table (Mean ¬± SD):\")\n",
    "display(df_elem_table)\n",
    "\n",
    "# ============================================================\n",
    "# Save formatted tables\n",
    "# ============================================================\n",
    "out_full = EVAL_DIR / \"summary_full_table_mean_only.csv\"\n",
    "out_elem = EVAL_DIR / \"summary_element_table_mean_sd.csv\"\n",
    "\n",
    "df_full_table.to_csv(out_full, index=False)\n",
    "df_elem_table.to_csv(out_elem, index=False)\n",
    "\n",
    "print(f\"\\nüíæ Saved formatted tables ‚Üí\\n‚Ä¢ {out_full}\\n‚Ä¢ {out_elem}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9519b49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nahid\\dmpchef\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "# If running from a notebook, ensure repo root is on sys.path (usually already true if notebook is at repo root)\n",
    "import sys\n",
    "REPO_ROOT = Path.cwd()\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "from src.core_pipeline import DMPPipeline\n",
    "from utils.dmptool_json import build_dmptool_json\n",
    "from utils.nih_docx_writer import build_nih_docx_from_template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21163508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_filename(title: str) -> str:\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", (title or \"\").strip()).strip()\n",
    "\n",
    "def ensure_dir(p: Path) -> None:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def cleanup_title_json(out_json_dir: Path, file_stem: str) -> None:\n",
    "    keep_name = f\"{file_stem}.dmptool.json\"\n",
    "    for p in out_json_dir.glob(f\"{file_stem}*.json\"):\n",
    "        if p.name != keep_name:\n",
    "            try:\n",
    "                p.unlink()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "def _to_bool(v, default: Optional[bool] = None) -> Optional[bool]:\n",
    "    if v is None:\n",
    "        return default\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if isinstance(v, (int, float)):\n",
    "        return bool(v)\n",
    "    if isinstance(v, str):\n",
    "        s = v.strip().lower()\n",
    "        if s in {\"true\", \"1\", \"yes\", \"y\", \"on\"}:\n",
    "            return True\n",
    "        if s in {\"false\", \"0\", \"no\", \"n\", \"off\"}:\n",
    "            return False\n",
    "    return default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "734fb35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draft(\n",
    "    input_json_path: str,\n",
    "    out_root: str = \"data/outputs\",\n",
    "    config_path: str = \"config/config.yaml\",\n",
    "    nih_template_path: str = \"data/inputs/nih-dms-plan-template.docx\",\n",
    "    use_rag: Optional[bool] = None,                # optional override (otherwise uses input.json / YAML default)\n",
    "    funding_agency: Optional[str] = None,          # optional override (otherwise uses input.json)\n",
    "    export_pdf: bool = True,                       # keep demo simple: try PDF, but don't fail if not supported\n",
    ") -> Dict[str, str]:\n",
    "    in_path = Path(input_json_path).expanduser().resolve()\n",
    "    if not in_path.exists():\n",
    "        raise FileNotFoundError(f\"Input JSON not found: {in_path}\")\n",
    "\n",
    "    req = json.loads(in_path.read_text(encoding=\"utf-8\"))\n",
    "    title = (req.get(\"title\") or \"\").strip()\n",
    "    inputs: Dict[str, Any] = req.get(\"inputs\") or {}\n",
    "\n",
    "    # Use request JSON unless overridden\n",
    "    req_funder = (req.get(\"funding_agency\") or \"NIH\").strip().upper()\n",
    "    funding_agency = (funding_agency or req_funder).strip().upper()\n",
    "\n",
    "    if not title:\n",
    "        raise ValueError(\"Input JSON must include a non-empty 'title'.\")\n",
    "\n",
    "    # Decide RAG usage (override > JSON > None => pipeline uses YAML default)\n",
    "    if use_rag is None and \"use_rag\" in req:\n",
    "        use_rag = _to_bool(req.get(\"use_rag\"), default=None)\n",
    "\n",
    "    # Output dirs\n",
    "    out_root = Path(out_root).expanduser().resolve()\n",
    "    out_json = out_root / \"json\"\n",
    "    out_md   = out_root / \"markdown\"\n",
    "    out_docx = out_root / \"docx\"\n",
    "    out_pdf  = out_root / \"pdf\"\n",
    "\n",
    "    for p in [out_json, out_md, out_docx, out_pdf]:\n",
    "        ensure_dir(p)\n",
    "\n",
    "    # Run pipeline (this returns markdown, and also writes md/docx/json internally)\n",
    "    pipeline = DMPPipeline(config_path=config_path, force_rebuild_index=False)\n",
    "\n",
    "    md_text = pipeline.generate_dmp(\n",
    "        title,\n",
    "        inputs,\n",
    "        use_rag=use_rag,\n",
    "        funding_agency=funding_agency,\n",
    "    )\n",
    "\n",
    "    run_stem = pipeline.last_run_stem or safe_filename(title)\n",
    "\n",
    "    # Save Markdown (not strictly necessary; pipeline already saves, but this keeps the demo explicit)\n",
    "    md_path = out_md / f\"{run_stem}.md\"\n",
    "    md_path.write_text(md_text, encoding=\"utf-8\")\n",
    "\n",
    "    # Save ONLY dmptool JSON (same approach as main.py)\n",
    "    dmptool_payload = build_dmptool_json(\n",
    "        template_title=\"NIH Data Management and Sharing Plan\",\n",
    "        project_title=title,\n",
    "        form_inputs=inputs,\n",
    "        generated_markdown=md_text,\n",
    "        provenance=\"dmpchef\",\n",
    "    )\n",
    "    dmptool_json_path = out_json / f\"{run_stem}.dmptool.json\"\n",
    "    cleanup_title_json(out_json, run_stem)\n",
    "    dmptool_json_path.write_text(json.dumps(dmptool_payload, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # Build NIH template DOCX (explicit for demo)\n",
    "    template_path = Path(nih_template_path).expanduser().resolve()\n",
    "    if not template_path.exists():\n",
    "        raise FileNotFoundError(f\"NIH template DOCX not found: {template_path}\")\n",
    "\n",
    "    docx_path = out_docx / f\"{run_stem}.docx\"\n",
    "    build_nih_docx_from_template(\n",
    "        template_docx_path=str(template_path),\n",
    "        output_docx_path=str(docx_path),\n",
    "        project_title=title,\n",
    "        generated_markdown=md_text,\n",
    "    )\n",
    "\n",
    "    # Optional PDF (best-effort; docx2pdf works best on Windows with Word installed)\n",
    "    pdf_path = out_pdf / f\"{run_stem}.pdf\"\n",
    "    if export_pdf:\n",
    "        try:\n",
    "            from docx2pdf import convert as docx2pdf_convert\n",
    "            pdf_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            docx2pdf_convert(str(docx_path), str(pdf_path))\n",
    "        except Exception as e:\n",
    "            print(\"⚠️ PDF conversion skipped (docx2pdf not supported in this environment).\")\n",
    "            print(\"   Reason:\", str(e))\n",
    "            pdf_path = None\n",
    "\n",
    "    return {\n",
    "        \"funding_agency\": funding_agency,\n",
    "        \"use_rag\": str(use_rag),\n",
    "        \"run_stem\": run_stem,\n",
    "        \"markdown\": str(md_path),\n",
    "        \"docx\": str(docx_path),\n",
    "        \"pdf\": str(pdf_path) if pdf_path else \"\",\n",
    "        \"dmptool_json\": str(dmptool_json_path),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d72b6370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"timestamp\": \"2026-02-07T01:35:15.695554Z\", \"level\": \"info\", \"event\": \"\\u2705 Config loaded successfully\"}\n",
      "{\"llm\": \"llama3.3\", \"embed\": \"sentence-transformers/all-MiniLM-L6-v2\", \"hf_cache_dir\": \"data/cache/hf\", \"local_files_only\": false, \"timestamp\": \"2026-02-07T01:35:15.697059Z\", \"level\": \"info\", \"event\": \"ModelLoader initialized\"}\n",
      "C:\\Users\\Nahid\\dmpchef\\src\\core_pipeline.py:119: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaLLM``.\n",
      "  self.llm = Ollama(model=self.llm_name)\n",
      "{\"llm\": \"llama3.3\", \"rag_default\": true, \"timestamp\": \"2026-02-07T01:35:15.698059Z\", \"level\": \"info\", \"event\": \"\\u2705 DMPPipeline initialized\"}\n",
      "{\"funding_agency\": \"NIH\", \"timestamp\": \"2026-02-07T01:35:15.698059Z\", \"level\": \"info\", \"event\": \"\\ud83c\\udff7\\ufe0f Funding agency selected\"}\n",
      "{\"use_rag_input\": true, \"rag_default\": true, \"use_rag_final\": true, \"timestamp\": \"2026-02-07T01:35:15.700059Z\", \"level\": \"info\", \"event\": \"\\ud83e\\uddfe RAG decision\"}\n",
      "C:\\Users\\Nahid\\dmpchef\\utils\\model_loader.py:69: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  emb = HuggingFaceEmbeddings(\n",
      "Use pytorch device_name: cpu\n",
      "Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json \"HTTP/1.1 404 Not Found\"\n",
      "HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json \"HTTP/1.1 200 OK\"\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1596.78it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-MiniLM-L6-v2 \"HTTP/1.1 200 OK\"\n",
      "{\"model\": \"sentence-transformers/all-MiniLM-L6-v2\", \"cache_dir\": \"C:\\\\Users\\\\Nahid\\\\dmpchef\\\\notebook_DMP_RAG\\\\data\\\\cache\\\\hf\", \"local_files_only\": false, \"timestamp\": \"2026-02-07T01:35:17.412345Z\", \"level\": \"info\", \"event\": \"Embeddings loaded successfully\"}\n",
      "{\"path\": \"data\\\\index\\\\index.faiss\", \"timestamp\": \"2026-02-07T01:35:17.412345Z\", \"level\": \"info\", \"event\": \"\\ud83d\\udce6 Loading existing FAISS index\"}\n",
      "Loading faiss with AVX512 support.\n",
      "Could not load library with AVX512 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx512'\")\n",
      "Loading faiss with AVX2 support.\n",
      "Successfully loaded faiss with AVX2 support.\n",
      "{\"type\": \"mmr\", \"top_k\": 10, \"timestamp\": \"2026-02-07T01:35:17.434342Z\", \"level\": \"info\", \"event\": \"\\u2705 RAG retriever initialized\"}\n",
      "{\"retriever_top_k\": 10, \"retrieved_docs\": 10, \"retrieved_chars\": 8979, \"debug_context_file\": \"data\\\\outputs\\\\debug\\\\NIH Data Management and Sharing Plan_ Clinical and MRI Data from Human Research Participants__rag__k10__llama3.3.retrieved_context.txt\", \"timestamp\": \"2026-02-07T01:35:17.469339Z\", \"level\": \"info\", \"event\": \"\\ud83d\\udd0e Retrieval complete\"}\n",
      "{\"title\": \"NIH Data Management and Sharing Plan: Clinical and MRI Data from Human Research Participants\", \"funding_agency\": \"NIH\", \"use_rag\": true, \"md\": \"data\\\\outputs\\\\markdown\\\\NIH Data Management and Sharing Plan_ Clinical and MRI Data from Human Research Participants__rag__k10__llama3.3.md\", \"docx\": \"data\\\\outputs\\\\docx\\\\NIH Data Management and Sharing Plan_ Clinical and MRI Data from Human Research Participants__rag__k10__llama3.3.docx\", \"dmptool_json\": \"data\\\\outputs\\\\json\\\\NIH Data Management and Sharing Plan_ Clinical and MRI Data from Human Research Participants__rag__k10__llama3.3.dmptool.json\", \"timestamp\": \"2026-02-07T01:36:54.023666Z\", \"level\": \"info\", \"event\": \"\\u2705 DMP generated successfully\"}\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.87s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'funding_agency': 'NIH',\n",
       " 'use_rag': 'True',\n",
       " 'run_stem': 'NIH Data Management and Sharing Plan_ Clinical and MRI Data from Human Research Participants__rag__k10__llama3.3',\n",
       " 'markdown': 'C:\\\\Users\\\\Nahid\\\\dmpchef\\\\notebook_DMP_RAG\\\\data\\\\outputs\\\\markdown\\\\NIH Data Management and Sharing Plan_ Clinical and MRI Data from Human Research Participants__rag__k10__llama3.3.md',\n",
       " 'docx': 'C:\\\\Users\\\\Nahid\\\\dmpchef\\\\notebook_DMP_RAG\\\\data\\\\outputs\\\\docx\\\\NIH Data Management and Sharing Plan_ Clinical and MRI Data from Human Research Participants__rag__k10__llama3.3.docx',\n",
       " 'pdf': 'C:\\\\Users\\\\Nahid\\\\dmpchef\\\\notebook_DMP_RAG\\\\data\\\\outputs\\\\pdf\\\\NIH Data Management and Sharing Plan_ Clinical and MRI Data from Human Research Participants__rag__k10__llama3.3.pdf',\n",
       " 'dmptool_json': 'C:\\\\Users\\\\Nahid\\\\dmpchef\\\\notebook_DMP_RAG\\\\data\\\\outputs\\\\json\\\\NIH Data Management and Sharing Plan_ Clinical and MRI Data from Human Research Participants__rag__k10__llama3.3.dmptool.json'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_paths = draft(\"data/inputs/input.json\", export_pdf=True)\n",
    "result_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d501d08e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0df47d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

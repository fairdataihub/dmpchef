{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e0f6b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start 10/08/2025\n"
     ]
    }
   ],
   "source": [
    "print (\"Start 10/08/2025\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fb52b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ STEP 1 ready\n",
      "ROOT_DIR   : c:\\Users\\Nahid\\dmpchef\n",
      "DATA_PDFS  : c:\\Users\\Nahid\\dmpchef\\data\\NIH_95\n",
      "INDEX_DIR  : c:\\Users\\Nahid\\dmpchef\\data\\faiss_index\n",
      "EXCEL_PATH : c:\\Users\\Nahid\\dmpchef\\data\\inputs\\inputs.xlsx\n",
      "TEMPLATE_MD: c:\\Users\\Nahid\\dmpchef\\data\\inputs\\dmp-template.md\n",
      "OUTPUT_MD  : c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\markdown\n",
      "OUTPUT_DOCX: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\n",
      "EMBED_MODEL: sentence-transformers/all-MiniLM-L6-v2 | LLM_MODEL: llama3.3 | TOP_K: 6\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 1 ‚Äî Imports, Config, and Helpers\n",
    "# ============================================\n",
    "import os, re, time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import pypandoc  # for Markdown ‚Üí DOCX\n",
    "\n",
    "# --- LangChain Core ---\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ---------- Paths (works in notebook or script) ----------\n",
    "try:\n",
    "    ROOT_DIR = Path(__file__).resolve().parents[1]  # when running a .py script\n",
    "except NameError:\n",
    "    ROOT_DIR = Path.cwd().parent                     # when running inside Jupyter\n",
    "\n",
    "# --- Data folders ---\n",
    "DATA_PDFS   = ROOT_DIR / \"data\" / \"NIH_95\"\n",
    "INDEX_DIR   = ROOT_DIR / \"data\" / \"faiss_index\"\n",
    "EXCEL_PATH  = ROOT_DIR / \"data\" / \"inputs\" / \"inputs.xlsx\"\n",
    "TEMPLATE_MD = ROOT_DIR / \"data\" / \"inputs\" / \"dmp-template.md\"\n",
    "\n",
    "# --- Output folders ---\n",
    "OUTPUT_MD   = ROOT_DIR / \"data\" / \"outputs1\" / \"markdown\"\n",
    "OUTPUT_DOCX = ROOT_DIR / \"data\" / \"outputs1\" / \"docx\"\n",
    "\n",
    "# --- Models / parameters ---\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "LLM_MODEL   = \"llama3.3\"\n",
    "TOP_K       = 6\n",
    "\n",
    "# ---------- Helper functions ----------\n",
    "def create_folder(folderpath):\n",
    "    Path(folderpath).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_md(folderpath, filename, text):\n",
    "    create_folder(folderpath)\n",
    "    (Path(folderpath) / filename).write_text(text, encoding=\"utf-8\")\n",
    "    print(\"üíæ Saved:\", Path(folderpath) / filename)\n",
    "\n",
    "def md_to_docs(md_filepath, docx_folderpath, docx_filename):\n",
    "    create_folder(docx_folderpath)\n",
    "    pypandoc.convert_file(\n",
    "        str(md_filepath), \"docx\",\n",
    "        outputfile=str(Path(docx_folderpath) / docx_filename)\n",
    "    )\n",
    "    print(\"üìÑ Converted:\", Path(docx_folderpath) / docx_filename)\n",
    "\n",
    "def clean_filename(name: str) -> str:\n",
    "    \"\"\"Remove illegal characters from filenames (Windows-safe).\"\"\"\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", str(name)).strip()\n",
    "\n",
    "# ---------- Ensure required folders exist ----------\n",
    "for p in [DATA_PDFS, INDEX_DIR, OUTPUT_MD, OUTPUT_DOCX]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Sanity print ----------\n",
    "print(\"‚úÖ STEP 1 ready\")\n",
    "print(f\"ROOT_DIR   : {ROOT_DIR}\")\n",
    "print(f\"DATA_PDFS  : {DATA_PDFS}\")\n",
    "print(f\"INDEX_DIR  : {INDEX_DIR}\")\n",
    "print(f\"EXCEL_PATH : {EXCEL_PATH}\")\n",
    "print(f\"TEMPLATE_MD: {TEMPLATE_MD}\")\n",
    "print(f\"OUTPUT_MD  : {OUTPUT_MD}\")\n",
    "print(f\"OUTPUT_DOCX: {OUTPUT_DOCX}\")\n",
    "print(f\"EMBED_MODEL: {EMBED_MODEL} | LLM_MODEL: {LLM_MODEL} | TOP_K: {TOP_K}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "687935b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üì• Loading PDFs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 105/105 [00:22<00:00,  4.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 586 pages from 105 PDFs.\n",
      "‚úÖ Created 2016 chunks from 586 pages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 2 ‚Äî Load PDFs and Split into Text Chunks\n",
    "# ============================================\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def load_pdfs_from_folder(folder: Path):\n",
    "    \"\"\"Load all PDF files from a folder into LangChain Document objects.\"\"\"\n",
    "    if not folder.exists():\n",
    "        raise FileNotFoundError(f\"‚ùå Folder not found: {folder}\")\n",
    "    pdf_files = sorted(folder.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(f\"‚ö†Ô∏è No PDF files found in {folder}\")\n",
    "\n",
    "    docs = []\n",
    "    for pdf_path in tqdm(pdf_files, desc=\"üì• Loading PDFs\"):\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_path))\n",
    "            docs.extend(loader.load())\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipped {pdf_path.name}: {e}\")\n",
    "\n",
    "    print(f\"‚úÖ Loaded {len(docs)} pages from {len(pdf_files)} PDFs.\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "def split_into_chunks(docs, chunk_size=800, chunk_overlap=120):\n",
    "    \"\"\"Split PDF text into overlapping chunks for embedding/indexing.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    print(f\"‚úÖ Created {len(chunks)} chunks from {len(docs)} pages.\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# --- Run quick test ---\n",
    "raw_docs = load_pdfs_from_folder(DATA_PDFS)\n",
    "chunks = split_into_chunks(raw_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a0f00cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 1607.01it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Existing FAISS index found. Loading from disk...\n",
      "‚úÖ FAISS index loaded successfully.\n",
      "‚úÖ Retriever ready (top_k=6)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# STEP 3 ‚Äî Build or Load FAISS Index\n",
    "# ============================================\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import time\n",
    "\n",
    "# --- Initialize embedding model ---\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "def build_or_load_faiss_index(index_dir=INDEX_DIR, chunks=None):\n",
    "    \"\"\"\n",
    "    Builds a new FAISS index from text chunks if none exists,\n",
    "    otherwise loads the saved one from disk.\n",
    "    \"\"\"\n",
    "    faiss_path = index_dir / \"index.faiss\"\n",
    "    pkl_path   = index_dir / \"index.pkl\"\n",
    "\n",
    "    # --- If index exists, load it ---\n",
    "    if faiss_path.exists() and pkl_path.exists():\n",
    "        print(\"üì¶ Existing FAISS index found. Loading from disk...\")\n",
    "        vectorstore = FAISS.load_local(\n",
    "            str(index_dir),\n",
    "            embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        print(\"‚úÖ FAISS index loaded successfully.\")\n",
    "        return vectorstore\n",
    "\n",
    "    # --- Otherwise, build new index ---\n",
    "    if chunks is None or len(chunks) == 0:\n",
    "        raise RuntimeError(\"‚ùå No chunks provided. Please run Step 2 first to load and split PDFs.\")\n",
    "\n",
    "    print(\"üß± Building new FAISS index...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        tqdm(chunks, desc=\"üî¢ Embedding text chunks\"),\n",
    "        embeddings\n",
    "    )\n",
    "\n",
    "    # --- Save the index ---\n",
    "    vectorstore.save_local(str(index_dir))\n",
    "    duration = time.time() - start_time\n",
    "\n",
    "    print(f\"üíæ Saved new FAISS index to {index_dir}\")\n",
    "    print(f\"‚è±Ô∏è Build completed in {duration/60:.2f} minutes ({duration:.1f} seconds)\")\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "# --- Execute step ---\n",
    "vectorstore = build_or_load_faiss_index(INDEX_DIR, chunks)\n",
    "retriever   = vectorstore.as_retriever(search_kwargs={\"k\": TOP_K})\n",
    "print(f\"‚úÖ Retriever ready (top_k={TOP_K})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1d9b2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Excel loaded successfully: 26 rows\n",
      "‚úÖ DMP Markdown template loaded.\n",
      "üîó RAG chain initialized with model: llama3.3\n",
      "‚úÖ RAG chain ready for generation.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# üß© STEP 4 ‚Äî Load Excel, Template, and Build RAG Chain (Fixed)\n",
    "# ============================================\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_community.llms import Ollama\n",
    "import pandas as pd\n",
    "\n",
    "# --- Load Excel file ---\n",
    "if not EXCEL_PATH.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Excel file not found: {EXCEL_PATH}\")\n",
    "\n",
    "df = pd.read_excel(EXCEL_PATH)\n",
    "print(f\"‚úÖ Excel loaded successfully: {len(df)} rows\")\n",
    "\n",
    "# --- Load Markdown Template ---\n",
    "if not TEMPLATE_MD.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Template file not found: {TEMPLATE_MD}\")\n",
    "\n",
    "dmp_template_text = TEMPLATE_MD.read_text(encoding=\"utf-8\")\n",
    "print(\"‚úÖ DMP Markdown template loaded.\")\n",
    "\n",
    "\n",
    "# --- Build RAG chain ---\n",
    "def build_rag_chain(retriever, llm_model=LLM_MODEL):\n",
    "    \"\"\"\n",
    "    Build a flexible RAG pipeline that retrieves context\n",
    "    and generates a context-grounded NIH DMP section.\n",
    "    \"\"\"\n",
    "    llm = Ollama(model=llm_model)\n",
    "\n",
    "    prompt_template = \"\"\"You are an expert biomedical data steward and grant writer.\n",
    "Create a high-quality NIH Data Management and Sharing Plan (DMSP)\n",
    "based on the retrieved NIH context and the user's query.\n",
    "\n",
    "----\n",
    "Context from NIH Repository:\n",
    "{context}\n",
    "\n",
    "----\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Use the context above and follow the NIH template structure. Write fluently and cohesively.\n",
    "\"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "    parser = StrOutputParser()\n",
    "\n",
    "    def format_docs(docs):\n",
    "        \"\"\"Format retrieved documents into clean text.\"\"\"\n",
    "        if not docs:\n",
    "            return \"\"\n",
    "        formatted = []\n",
    "        for d in docs:\n",
    "            page = d.metadata.get(\"page\", \"\")\n",
    "            title = d.metadata.get(\"source\", \"\")\n",
    "            formatted.append(f\"[Page {page}] {title}\\n{d.page_content.strip()}\")\n",
    "        return \"\\n\\n\".join(formatted)\n",
    "\n",
    "    rag_chain = (\n",
    "        {\n",
    "            \"context\": retriever | format_docs,\n",
    "            \"question\": RunnablePassthrough()\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | parser\n",
    "    )\n",
    "\n",
    "    print(f\"üîó RAG chain initialized with model: {llm_model}\")\n",
    "    return rag_chain\n",
    "\n",
    "\n",
    "# --- Initialize the RAG chain ---\n",
    "rag_chain = build_rag_chain(retriever)\n",
    "print(\"‚úÖ RAG chain ready for generation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dd1758e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded input Excel ‚Äî 26 rows from: c:\\Users\\Nahid\\dmpchef\\data\\inputs\\inputs.xlsx\n",
      "‚úÖ Loaded NIH DMP Markdown template from: c:\\Users\\Nahid\\dmpchef\\data\\inputs\\dmp-template.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:   0%|          | 0/26 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß© Generating DMP for: Clinical and MRI data from human research participants\n",
      "üîé Retrieved 6 context chunks (using top 6).\n",
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Clinical and MRI data from human research participants.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Clinical and MRI data from human research participants.docx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:   4%|‚ñç         | 1/26 [01:19<33:18, 79.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß© Generating DMP for: Genomic data from human research participants\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:   8%|‚ñä         | 2/26 [02:30<29:51, 74.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Genomic data from human research participants.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Genomic data from human research participants.docx\n",
      "\n",
      "üß© Generating DMP for: Genomic data from a non-human source\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  12%|‚ñà‚ñè        | 3/26 [03:41<27:54, 72.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Genomic data from a non-human source.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Genomic data from a non-human source.docx\n",
      "\n",
      "üß© Generating DMP for: Secondary data analysis\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  15%|‚ñà‚ñå        | 4/26 [04:52<26:22, 71.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Secondary data analysis.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Secondary data analysis.docx\n",
      "\n",
      "üß© Generating DMP for: Human clinical and genomics data\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  19%|‚ñà‚ñâ        | 5/26 [06:04<25:15, 72.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Human clinical and genomics data.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Human clinical and genomics data.docx\n",
      "\n",
      "üß© Generating DMP for: Gene expression analysis data from non-human model organism (zebrafish)\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  23%|‚ñà‚ñà‚ñé       | 6/26 [07:25<25:05, 75.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Gene expression analysis data from non-human model organism (zebrafish).md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Gene expression analysis data from non-human model organism (zebrafish).docx\n",
      "\n",
      "üß© Generating DMP for: Human survey data\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  27%|‚ñà‚ñà‚ñã       | 7/26 [08:27<22:24, 70.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Human survey data.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Human survey data.docx\n",
      "\n",
      "üß© Generating DMP for: Clinical Data from Human Research Participants\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  31%|‚ñà‚ñà‚ñà       | 8/26 [09:37<21:09, 70.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Clinical Data from Human Research Participants.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Clinical Data from Human Research Participants.docx\n",
      "\n",
      "üß© Generating DMP for: Human genomic data\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [10:46<19:51, 70.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Human genomic data.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Human genomic data.docx\n",
      "\n",
      "üß© Generating DMP for: Technology development\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [12:02<19:08, 71.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Technology development.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Technology development.docx\n",
      "\n",
      "üß© Generating DMP for: Basic Research from a Non-Human Source Example\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 11/26 [13:20<18:25, 73.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Basic Research from a Non-Human Source Example.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Basic Research from a Non-Human Source Example.docx\n",
      "\n",
      "üß© Generating DMP for: Secondary Data Analysis Example\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [14:33<17:08, 73.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Secondary Data Analysis Example.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Secondary Data Analysis Example.docx\n",
      "\n",
      "üß© Generating DMP for: Survey and Interview Example\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 13/26 [15:39<15:26, 71.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Survey and Interview Example.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Survey and Interview Example.docx\n",
      "\n",
      "üß© Generating DMP for: Human Clinical Trial Data\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [16:44<13:54, 69.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Human Clinical Trial Data.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Human Clinical Trial Data.docx\n",
      "\n",
      "üß© Generating DMP for: Clinical data from human research participants-NIA\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [17:53<12:41, 69.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Clinical data from human research participants-NIA.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Clinical data from human research participants-NIA.docx\n",
      "\n",
      "üß© Generating DMP for: Survey, interview, and biological data (tiered access)\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [18:55<11:10, 67.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Survey, interview, and biological data (tiered access).md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Survey, interview, and biological data (tiered access).docx\n",
      "\n",
      "üß© Generating DMP for: Non-human data (primates)\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [19:57<09:49, 65.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Non-human data (primates).md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Non-human data (primates).docx\n",
      "\n",
      "üß© Generating DMP for: Secondary data analysis-NIA\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [21:07<08:55, 66.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Secondary data analysis-NIA.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Secondary data analysis-NIA.docx\n",
      "\n",
      "üß© Generating DMP for: Survey and interview data-NIA\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [22:06<07:31, 64.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Survey and interview data-NIA.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Survey and interview data-NIA.docx\n",
      "\n",
      "üß© Generating DMP for: Human clinical and genomic data-NIA\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [23:26<06:54, 69.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Human clinical and genomic data-NIA.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Human clinical and genomic data-NIA.docx\n",
      "\n",
      "üß© Generating DMP for: Non-human data (rodents)-NIA\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [24:41<05:54, 70.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Non-human data (rodents)-NIA.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Non-human data (rodents)-NIA.docx\n",
      "\n",
      "üß© Generating DMP for: Clinical data (human biospecimens)\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [26:01<04:54, 73.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Clinical data (human biospecimens).md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Clinical data (human biospecimens).docx\n",
      "\n",
      "üß© Generating DMP for: Drug discovery including intellectual property\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [27:08<03:35, 71.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Drug discovery including intellectual property.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Drug discovery including intellectual property.docx\n",
      "\n",
      "üß© Generating DMP for: HeLa Cell Whole Genome Sequence (DNA or RNA)\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 24/26 [28:11<02:18, 69.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\HeLa Cell Whole Genome Sequence (DNA or RNA).md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\HeLa Cell Whole Genome Sequence (DNA or RNA).docx\n",
      "\n",
      "üß© Generating DMP for: Secondary Data Analysis on Data from Human Subjects-NIA\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [29:23<01:09, 69.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Secondary Data Analysis on Data from Human Subjects-NIA.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Secondary Data Analysis on Data from Human Subjects-NIA.docx\n",
      "\n",
      "üß© Generating DMP for: Analysis of social media posts\n",
      "üîé Retrieved 6 context chunks (using top 6).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üß† Generating NIH DMPs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [30:27<00:00, 70.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\\Analysis of social media posts.md\n",
      "üìÑ Converted: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\\Analysis of social media posts.docx\n",
      "\n",
      "‚úÖ Finished generating NIH DMPs ‚Äî titles preserved from Excel!\n",
      "üìä CSV log saved to: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\rag_generated_dmp_log.csv\n",
      "üìÅ MD outputs:   c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\n",
      "üìÅ DOCX outputs: c:\\Users\\Nahid\\dmpchef\\data\\outputs1\\docx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# üß© STEP 5 ‚Äî RAG-Based DMP Generation Using Titles (CLEAN + ROBUST)\n",
    "# ============================================\n",
    "import re\n",
    "import pandas as pd\n",
    "import pypandoc\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- REQUIRED (assumed already defined earlier in your notebook/script) ----------\n",
    "# ROOT_DIR: Path\n",
    "# TEMPLATE_MD: Path\n",
    "# retriever: LangChain retriever (FAISS retriever, etc.)\n",
    "# rag_chain: your RAG chain (supports .invoke(prompt) -> str)\n",
    "\n",
    "# ---------- Paths ----------\n",
    "EXCEL_PATH = ROOT_DIR / \"data\" / \"inputs\" / \"inputs.xlsx\"\n",
    "\n",
    "OUTPUT_DIR = ROOT_DIR / \"data\" / \"outputs1\"\n",
    "OUTPUT_MD = OUTPUT_DIR / \"md\"\n",
    "OUTPUT_DOCX = OUTPUT_DIR / \"docx\"\n",
    "OUTPUT_LOG = OUTPUT_DIR / \"rag_generated_dmp_log.csv\"\n",
    "\n",
    "OUTPUT_MD.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUT_DOCX.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Load Excel ----------\n",
    "df = pd.read_excel(EXCEL_PATH)\n",
    "print(f\"‚úÖ Loaded input Excel ‚Äî {len(df)} rows from: {EXCEL_PATH}\")\n",
    "\n",
    "# Normalize column names\n",
    "df.columns = df.columns.str.strip().str.lower()\n",
    "df = df.fillna(\"\")\n",
    "\n",
    "# ---------- Validate required columns ----------\n",
    "if \"title\" not in df.columns:\n",
    "    raise ValueError(\"‚ùå Excel must contain a 'title' column (case-insensitive).\")\n",
    "\n",
    "element_cols = [c for c in df.columns if c.startswith(\"element\")]\n",
    "if not element_cols:\n",
    "    raise ValueError(\"‚ùå Excel must contain at least one column starting with 'element' (e.g., element1, element_2).\")\n",
    "\n",
    "# ---------- Verify template ----------\n",
    "if not TEMPLATE_MD.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Template not found: {TEMPLATE_MD}\")\n",
    "\n",
    "dmp_template_text = TEMPLATE_MD.read_text(encoding=\"utf-8\")\n",
    "print(f\"‚úÖ Loaded NIH DMP Markdown template from: {TEMPLATE_MD}\")\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def sanitize_filename(name: str) -> str:\n",
    "    \"\"\"Replace illegal filename characters but preserve readable title.\"\"\"\n",
    "    name = (name or \"\").strip()\n",
    "    name = re.sub(r\"\\s+\", \" \", name)  # collapse whitespace\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", name)\n",
    "\n",
    "def save_text(path: Path, content: str):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(content, encoding=\"utf-8\")\n",
    "\n",
    "def md_to_docx(md_path: Path, docx_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Convert Markdown to DOCX using Pandoc (via pypandoc).\n",
    "    Returns empty string on success, error message on failure.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        docx_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        pypandoc.convert_file(str(md_path), \"docx\", outputfile=str(docx_path))\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "def retrieve_context(query: str, top_k: int) -> tuple[str, int, str]:\n",
    "    \"\"\"\n",
    "    Returns (context_text, retrieved_count, error_message).\n",
    "    Uses retriever.invoke(query) if present; otherwise get_relevant_documents(query).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if hasattr(retriever, \"invoke\"):\n",
    "            docs = retriever.invoke(query)\n",
    "        else:\n",
    "            docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "        docs = docs or []\n",
    "        context_text = \"\\n\\n\".join(getattr(d, \"page_content\", str(d)) for d in docs[:top_k])\n",
    "        return context_text, len(docs), \"\"\n",
    "    except Exception as e:\n",
    "        return \"\", 0, str(e)\n",
    "\n",
    "# ---------- Main Generation ----------\n",
    "records = []\n",
    "TOP_K = 6\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"üß† Generating NIH DMPs\"):\n",
    "    raw_title = str(row.get(\"title\", \"\")).strip()\n",
    "\n",
    "    # If title is missing, create a fallback\n",
    "    if not raw_title:\n",
    "        raw_title = f\"Untitled_Project_Row_{idx+1}\"\n",
    "\n",
    "    print(f\"\\nüß© Generating DMP for: {raw_title}\")\n",
    "\n",
    "    # 1Ô∏è‚É£ Build query from Excel elements\n",
    "    element_texts = []\n",
    "    for col in element_cols:\n",
    "        val = str(row.get(col, \"\")).strip()\n",
    "        if val:\n",
    "            element_texts.append(f\"{col.upper()}: {val}\")\n",
    "    query_data = \"\\n\".join(element_texts)\n",
    "\n",
    "    query = (\n",
    "        \"You are an expert biomedical data steward and grant writer. \"\n",
    "        f\"Create a complete NIH Data Management and Sharing Plan (DMSP) for the project titled '{raw_title}'. \"\n",
    "        \"Use retrieved context from the NIH corpus to fill in all template sections accurately.\\n\\n\"\n",
    "        f\"Here is background information from the proposal:\\n{query_data}\\n\"\n",
    "    )\n",
    "\n",
    "    # 2Ô∏è‚É£ Retrieve context\n",
    "    context_text, retrieved_n, retrieval_error = retrieve_context(query, TOP_K)\n",
    "    if retrieval_error:\n",
    "        print(f\"‚ö†Ô∏è Retrieval failed for {raw_title}: {retrieval_error}\")\n",
    "    else:\n",
    "        print(f\"üîé Retrieved {retrieved_n} context chunks (using top {TOP_K}).\")\n",
    "\n",
    "    # 3Ô∏è‚É£ Combine context, query, and template\n",
    "    full_prompt = f\"\"\"\n",
    "You are an expert biomedical data steward and grant writer.\n",
    "Use the retrieved NIH context and the provided template to generate a complete Data Management and Sharing Plan.\n",
    "\n",
    "----\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "----\n",
    "Project Query:\n",
    "{query}\n",
    "\n",
    "Use the following NIH DMSP Markdown template. Do not alter section titles:\n",
    "{dmp_template_text}\n",
    "\"\"\".strip()\n",
    "\n",
    "    # 4Ô∏è‚É£ Generate with RAG chain\n",
    "    error_msg = \"\"\n",
    "    response_text = \"\"\n",
    "\n",
    "    try:\n",
    "        response = rag_chain.invoke(full_prompt)\n",
    "\n",
    "        # Normalize response to string (some chains return dict/AIMessage)\n",
    "        if isinstance(response, str):\n",
    "            response_text = response\n",
    "        elif hasattr(response, \"content\"):\n",
    "            response_text = response.content\n",
    "        elif isinstance(response, dict):\n",
    "            # common keys: \"output_text\", \"text\", \"result\"\n",
    "            response_text = response.get(\"output_text\") or response.get(\"text\") or response.get(\"result\") or str(response)\n",
    "        else:\n",
    "            response_text = str(response)\n",
    "\n",
    "        # 5Ô∏è‚É£ Save files with title-based filename\n",
    "        safe_title = sanitize_filename(raw_title)\n",
    "        md_path = OUTPUT_MD / f\"{safe_title}.md\"\n",
    "        docx_path = OUTPUT_DOCX / f\"{safe_title}.docx\"\n",
    "\n",
    "        save_text(md_path, response_text)\n",
    "        print(f\"üíæ Saved: {md_path}\")\n",
    "\n",
    "        docx_err = md_to_docx(md_path, docx_path)\n",
    "        if docx_err:\n",
    "            print(f\"‚ö†Ô∏è DOCX conversion failed (Pandoc?): {docx_err}\")\n",
    "            error_msg = f\"DOCX conversion failed: {docx_err}\"\n",
    "        else:\n",
    "            print(f\"üìÑ Converted: {docx_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        print(f\"‚ùå Error generating DMP for {raw_title}: {error_msg}\")\n",
    "\n",
    "    # 6Ô∏è‚É£ Log summary (always)\n",
    "    records.append({\n",
    "        \"Title\": raw_title,\n",
    "        \"Query\": query,\n",
    "        \"Retrieved_Context\": (context_text or \"\")[:1000],\n",
    "        \"Generated_DMP_Preview\": (response_text or \"\")[:1000],\n",
    "        \"Retrieval_Error\": retrieval_error,\n",
    "        \"Error\": error_msg\n",
    "    })\n",
    "\n",
    "# ---------- Save Log ----------\n",
    "pd.DataFrame(records).to_csv(OUTPUT_LOG, index=False, encoding=\"utf-8\")\n",
    "print(\"\\n‚úÖ Finished generating NIH DMPs ‚Äî titles preserved from Excel!\")\n",
    "print(f\"üìä CSV log saved to: {OUTPUT_LOG}\")\n",
    "print(f\"üìÅ MD outputs:   {OUTPUT_MD}\")\n",
    "print(f\"üìÅ DOCX outputs: {OUTPUT_DOCX}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db58c3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ ROOT_DIR set to: C:\\Users\\Nahid\\dmpchef\n",
      "üìó Gold PDF folder: C:\\Users\\Nahid\\dmpchef\\data\\inputs\\gold_dmps\n",
      "üìò Generated Markdown folder: C:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\n",
      "üìô Evaluation output folder: C:\\Users\\Nahid\\dmpchef\\data\\outputs1\\evaluation_results\n",
      "üöÄ Loading models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 1634.62it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Models ready.\n",
      "üìä Found 26 generated DMPs and 26 gold PDFs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Matching & Comparing DMPs:   4%|‚ñç         | 1/26 [00:00<00:05,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matched Analysis of social media posts.md ‚Üî 26-Analysis of social media posts-NCI.pdf (score=0.90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Matching & Comparing DMPs:   8%|‚ñä         | 2/26 [00:00<00:05,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matched Basic Research from a Non-Human Source Example.md ‚Üî 11-Basic Research from a Non-Human Source Example-NIDDK.pdf (score=0.91)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Matching & Comparing DMPs:  12%|‚ñà‚ñè        | 3/26 [00:00<00:06,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matched Clinical and MRI data from human research participants.md ‚Üî 1-Clinical andor MRI data from human research participants-NIMH.pdf (score=0.92)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Matching & Comparing DMPs:  15%|‚ñà‚ñå        | 4/26 [00:01<00:06,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matched Clinical data (human biospecimens).md ‚Üî 22-Clinical data (human biospecimens)-NIA.pdf (score=0.90)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Matching & Comparing DMPs:  19%|‚ñà‚ñâ        | 5/26 [00:01<00:05,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matched Clinical data from human research participants-NIA.md ‚Üî 15-Clinical data from human research participants-NIA.pdf (score=0.97)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Matching & Comparing DMPs:  23%|‚ñà‚ñà‚ñé       | 6/26 [00:01<00:05,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matched Clinical Data from Human Research Participants.md ‚Üî 15-Clinical data from human research participants-NIA.pdf (score=0.93)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Matching & Comparing DMPs:  27%|‚ñà‚ñà‚ñã       | 7/26 [00:01<00:04,  3.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matched Drug discovery including intellectual property.md ‚Üî 23-Drug discovery including intellectual property-NIA.pdf (score=0.93)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Matching & Comparing DMPs:  31%|‚ñà‚ñà‚ñà       | 8/26 [00:02<00:04,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matched Gene expression analysis data from non-human model organism (zebrafish).md ‚Üî 8-Gene expression analysis data from non-human model organism (zebrafish)-NICHD.pdf (score=0.95)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Matching & Comparing DMPs:  35%|‚ñà‚ñà‚ñà‚ñç      | 9/26 [00:02<00:04,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matched Genomic data from a non-human source.md ‚Üî 3-Genomic data from a non-human source-NIMH.pdf (score=0.91)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Matching & Comparing DMPs:  38%|‚ñà‚ñà‚ñà‚ñä      | 10/26 [00:02<00:04,  3.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matched Genomic data from human research participants.md ‚Üî 2-Genomic data from human research participants-NIMH.pdf (score=0.93)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Matching & Comparing DMPs:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 11/26 [00:02<00:03,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matched HeLa Cell Whole Genome Sequence (DNA or RNA).md ‚Üî 24-HeLa Cell Whole Genome Sequence (DNA or RNA)-OD, NHGRI.pdf (score=0.88)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Matching & Comparing DMPs:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [00:03<00:03,  3.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matched Human clinical and genomic data-NIA.md ‚Üî 20-Human clinical and genomic data-NIA.pdf (score=0.96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Matching & Comparing DMPs:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 13/26 [00:03<00:03,  3.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matched Human clinical and genomics data.md ‚Üî 7-Human clinical and genomics data-NICHD.pdf (score=0.89)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Matching & Comparing DMPs:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [00:03<00:03,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matched Human Clinical Trial Data.md ‚Üî 14-Human Clinical Trial Data-NICHD.pdf (score=0.85)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Matching & Comparing DMPs:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [00:04<00:03,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matched Human genomic data.md ‚Üî 5-Human genomic data-NHGRI.pdf (score=0.82)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Matching & Comparing DMPs:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [00:04<00:02,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matched Human survey data.md ‚Üî 9-Human survey data-NICHD.pdf (score=0.81)\n",
      "‚úÖ Matched Non-human data (primates).md ‚Üî 17-Non-human data (primates)-NIA.pdf (score=0.87)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Matching & Comparing DMPs:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [00:04<00:02,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matched Non-human data (rodents)-NIA.md ‚Üî 21-Non-human data (rodents)-NIA.pdf (score=0.95)\n",
      "‚úÖ Matched Secondary Data Analysis Example.md ‚Üî 12-Secondary Data Analysis Example-NIDDK.pdf (score=0.87)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Matching & Comparing DMPs:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [00:05<00:01,  4.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matched Secondary Data Analysis on Data from Human Subjects-NIA.md ‚Üî 25-Secondary Data Analysis on Data from Human Subjects-NIA.pdf (score=0.97)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Matching & Comparing DMPs:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [00:05<00:01,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matched Secondary data analysis-NIA.md ‚Üî 18-Secondary data analysis-NIA.pdf (score=0.95)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Matching & Comparing DMPs:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [00:05<00:00,  4.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matched Secondary data analysis.md ‚Üî 18-Secondary data analysis-NIA.pdf (score=0.87)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Matching & Comparing DMPs:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 23/26 [00:06<00:00,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matched Survey and interview data-NIA.md ‚Üî 19-Survey and interview data-NIA.pdf (score=0.95)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Matching & Comparing DMPs:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 25/26 [00:06<00:00,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matched Survey and Interview Example.md ‚Üî 13-Survey and Interview Example-NHGRI.pdf (score=0.86)\n",
      "‚úÖ Matched Survey, interview, and biological data (tiered access).md ‚Üî 16-Survey, interview, and biological data (tiered access)-NIA.pdf (score=0.93)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîé Matching & Comparing DMPs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:06<00:00,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Matched Technology development.md ‚Üî 6-Technology development-NHGRI.pdf (score=0.85)\n",
      "\n",
      "‚úÖ Markdown‚ÄìPDF (fuzzy) similarity results saved to: C:\\Users\\Nahid\\dmpchef\\data\\outputs1\\evaluation_results\\full_dmp_pdf_comparison_fuzzy.csv\n",
      "üßæ Total matched DMP pairs: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# üß© STEP 6 ‚Äî Full DMP Comparison: Markdown (Generated) vs PDF (Gold, Fuzzy Matching)\n",
    "#         (ROBUST ROOT DETECTION + outputs1 paths + faster SBERT caching)\n",
    "# ============================================\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from difflib import SequenceMatcher\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# ---------------------------\n",
    "# ‚úÖ Robust project root finder\n",
    "# ---------------------------\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Walk upward from `start` and return the first directory that looks like the project root.\n",
    "    Heuristics: contains data/ and (README.md or config/).\n",
    "    \"\"\"\n",
    "    cur = start.resolve()\n",
    "    for _ in range(25):  # safety limit\n",
    "        data_dir = cur / \"data\"\n",
    "        has_data = data_dir.exists() and data_dir.is_dir()\n",
    "        has_readme = (cur / \"README.md\").exists()\n",
    "        has_config = (cur / \"config\").exists() and (cur / \"config\").is_dir()\n",
    "        # Primary: data/ + (README.md or config/)\n",
    "        if has_data and (has_readme or has_config):\n",
    "            return cur\n",
    "        # Secondary: exact folder name match (keep your old behavior too)\n",
    "        if cur.name.lower() == \"dmp-rag\":\n",
    "            return cur\n",
    "        if cur.parent == cur:\n",
    "            break\n",
    "        cur = cur.parent\n",
    "    return start.resolve()\n",
    "\n",
    "ROOT_DIR = find_project_root(Path.cwd())\n",
    "print(f\"üìÇ ROOT_DIR set to: {ROOT_DIR}\")\n",
    "\n",
    "# --- Paths (match STEP 5/7 layout) ---\n",
    "GOLD_DIR      = ROOT_DIR / \"data\" / \"inputs\" / \"gold_dmps\"\n",
    "GENERATED_DIR = ROOT_DIR / \"data\" / \"outputs1\" / \"md\"\n",
    "EVAL_DIR      = ROOT_DIR / \"data\" / \"outputs1\" / \"evaluation_results\"\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìó Gold PDF folder: {GOLD_DIR}\")\n",
    "print(f\"üìò Generated Markdown folder: {GENERATED_DIR}\")\n",
    "print(f\"üìô Evaluation output folder: {EVAL_DIR}\")\n",
    "\n",
    "# --- Models ---\n",
    "print(\"üöÄ Loading models...\")\n",
    "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "print(\"‚úÖ Models ready.\")\n",
    "\n",
    "# --- Helper functions ---\n",
    "def normalize_name(name: str) -> str:\n",
    "    name = (name or \"\").lower()\n",
    "    name = re.sub(r\"[^a-z0-9\\s]\", \" \", name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name).strip()\n",
    "    return name\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL)\n",
    "    text = re.sub(r\"```.*?```\", \"\", text, flags=re.DOTALL)\n",
    "    text = re.sub(r\"#+\\s*\", \"\", text)\n",
    "    text = re.sub(r\"\\*\\*|\\*\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: Path) -> str:\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text(\"text\") + \"\\n\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading {pdf_path.name}: {e}\")\n",
    "    return clean_text(text)\n",
    "\n",
    "def chunk_text(text: str, size_words: int = 300):\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return []\n",
    "    return [\" \".join(words[i:i + size_words]) for i in range(0, len(words), size_words)]\n",
    "\n",
    "def best_fuzzy_match(target: str, gold_names: list[str], threshold: float = 0.6):\n",
    "    best_match, best_score = None, 0.0\n",
    "    for g in gold_names:\n",
    "        score = SequenceMatcher(None, target, g).ratio()\n",
    "        if score > best_score:\n",
    "            best_match, best_score = g, score\n",
    "    return (best_match, best_score) if best_score >= threshold else (None, best_score)\n",
    "\n",
    "def compare_chunked_cached(gold_text: str, gen_text: str, model: SentenceTransformer):\n",
    "    gold_chunks = chunk_text(gold_text, size_words=300)\n",
    "    gen_chunks  = chunk_text(gen_text,  size_words=300)\n",
    "    if not gold_chunks or not gen_chunks:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    gold_emb = model.encode(gold_chunks, convert_to_tensor=True)\n",
    "    gen_emb  = model.encode(gen_chunks,  convert_to_tensor=True)\n",
    "\n",
    "    sbert_scores = []\n",
    "    for i in range(len(gold_chunks)):\n",
    "        sims = util.cos_sim(gold_emb[i], gen_emb)[0]\n",
    "        sbert_scores.append(float(sims.max().item()))\n",
    "\n",
    "    rouge_scores = []\n",
    "    for g in gold_chunks:\n",
    "        best_r = 0.0\n",
    "        for gen in gen_chunks:\n",
    "            r = rouge.score(g, gen)[\"rougeL\"].recall\n",
    "            if r > best_r:\n",
    "                best_r = r\n",
    "        rouge_scores.append(best_r)\n",
    "\n",
    "    return float(np.mean(sbert_scores)), float(np.mean(rouge_scores))\n",
    "\n",
    "# --- Validate folders with helpful diagnostics ---\n",
    "if not GOLD_DIR.exists():\n",
    "    print(\"\\n‚ùå GOLD_DIR not found.\")\n",
    "    print(\"üîé Debug tips:\")\n",
    "    print(f\" - Your current working dir is: {Path.cwd()}\")\n",
    "    print(f\" - Detected ROOT_DIR is:       {ROOT_DIR}\")\n",
    "    print(\" - Expected gold PDFs under:   data/inputs/gold_dmps/\")\n",
    "    # show likely alternatives for user\n",
    "    data_inputs = ROOT_DIR / \"data\" / \"inputs\"\n",
    "    if data_inputs.exists():\n",
    "        print(f\"‚úÖ Found data/inputs at: {data_inputs}\")\n",
    "        print(\"üìÅ Contents of data/inputs/:\")\n",
    "        for p in sorted(data_inputs.iterdir()):\n",
    "            print(\"   -\", p.name)\n",
    "    raise FileNotFoundError(f\"‚ùå GOLD_DIR not found: {GOLD_DIR}\")\n",
    "\n",
    "if not GENERATED_DIR.exists():\n",
    "    print(\"\\n‚ùå GENERATED_DIR not found.\")\n",
    "    print(\"üîé Debug tips:\")\n",
    "    print(f\" - Expected generated MDs under: {GENERATED_DIR}\")\n",
    "    raise FileNotFoundError(f\"‚ùå GENERATED_DIR not found: {GENERATED_DIR}\")\n",
    "\n",
    "# --- Collect gold PDFs and generated MDs ---\n",
    "gold_files = {normalize_name(f.stem): f for f in GOLD_DIR.glob(\"*.pdf\")}\n",
    "gen_files  = {normalize_name(f.stem): f for f in GENERATED_DIR.glob(\"*.md\")}\n",
    "\n",
    "print(f\"üìä Found {len(gen_files)} generated DMPs and {len(gold_files)} gold PDFs.\")\n",
    "\n",
    "# --- Compare all matching files ---\n",
    "results = []\n",
    "for name, gen_path in tqdm(gen_files.items(), desc=\"üîé Matching & Comparing DMPs\"):\n",
    "    best_match, match_score = best_fuzzy_match(name, list(gold_files.keys()), threshold=0.6)\n",
    "    if not best_match:\n",
    "        print(f\"‚ö†Ô∏è No gold match for: {gen_path.name}\")\n",
    "        continue\n",
    "\n",
    "    gold_path = gold_files[best_match]\n",
    "    gold_text = extract_text_from_pdf(gold_path)\n",
    "    gen_text  = clean_text(gen_path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "\n",
    "    if not gold_text.strip() or not gen_text.strip():\n",
    "        print(f\"‚ö†Ô∏è Skipping empty text pair: {gen_path.name} ‚Üî {gold_path.name}\")\n",
    "        continue\n",
    "\n",
    "    sbert_sim, rouge_l = compare_chunked_cached(gold_text, gen_text, sbert)\n",
    "\n",
    "    results.append({\n",
    "        \"Generated_File\": gen_path.name,\n",
    "        \"Matched_Gold_PDF\": gold_path.name,\n",
    "        \"Match_Score\": round(float(match_score), 3),\n",
    "        \"SBERT_Similarity\": round(float(sbert_sim), 4) if not np.isnan(sbert_sim) else np.nan,\n",
    "        \"ROUGE_L_Recall\": round(float(rouge_l), 4) if not np.isnan(rouge_l) else np.nan,\n",
    "        \"Generated_Path\": str(gen_path),\n",
    "        \"Gold_Path\": str(gold_path),\n",
    "    })\n",
    "\n",
    "    print(f\"‚úÖ Matched {gen_path.name} ‚Üî {gold_path.name} (score={match_score:.2f})\")\n",
    "\n",
    "# --- Save results ---\n",
    "df_results = pd.DataFrame(results)\n",
    "out_path = EVAL_DIR / \"full_dmp_pdf_comparison_fuzzy.csv\"\n",
    "df_results.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\n‚úÖ Markdown‚ÄìPDF (fuzzy) similarity results saved to: {out_path}\")\n",
    "print(f\"üßæ Total matched DMP pairs: {len(df_results)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c307ea5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ ROOT_DIR set to: C:\\Users\\Nahid\\dmpchef\n",
      "üìç CWD is: c:\\Users\\Nahid\\dmpchef\\notebook_DMP_RAG\n",
      "üìó Gold Excel: C:\\Users\\Nahid\\dmpchef\\data\\inputs\\inputs.xlsx\n",
      "üìò Generated MD folder: C:\\Users\\Nahid\\dmpchef\\data\\outputs1\\md\n",
      "üìô Evaluation folder: C:\\Users\\Nahid\\dmpchef\\data\\outputs1\\evaluation_results\n",
      "‚úÖ Loaded 26 gold projects.\n",
      "üß© Detected 12 element columns: ['element_1a', 'element_1b', 'element_1c', 'element_2', 'element_3', 'element_4a', 'element_4b', 'element_4c', 'element_5a', 'element_5b', 'element_5c', 'element_6']\n",
      "üöÄ Loading evaluation models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 1633.93it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Models ready.\n",
      "üîç Found 26 generated Markdown files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üìä Comparing element-level: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:04<00:00,  5.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Element-level similarity saved to: C:\\Users\\Nahid\\dmpchef\\data\\outputs1\\evaluation_results\\element_similarity_exact_titles.csv\n",
      "üßæ Total element‚Äìsection best matches: 312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# üß© STEP 7 ‚Äî Element-Level Comparison with NIH Gold Standard (Exact Title Match)\n",
    "#        (ROBUST ROOT DETECTION + outputs1 paths + faster embedding cache)\n",
    "# ============================================\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# ---------------------------\n",
    "# ‚úÖ Robust project root finder\n",
    "# ---------------------------\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Walk upward from `start` and return the first directory that looks like the project root.\n",
    "    Heuristics: contains data/ and (README.md or config/).\n",
    "    Also accepts folder name 'DMP-RAG' if present.\n",
    "    \"\"\"\n",
    "    cur = start.resolve()\n",
    "    for _ in range(25):\n",
    "        data_dir = cur / \"data\"\n",
    "        has_data = data_dir.exists() and data_dir.is_dir()\n",
    "        has_readme = (cur / \"README.md\").exists()\n",
    "        has_config = (cur / \"config\").exists() and (cur / \"config\").is_dir()\n",
    "\n",
    "        if has_data and (has_readme or has_config):\n",
    "            return cur\n",
    "        if cur.name.lower() == \"dmp-rag\":\n",
    "            return cur\n",
    "\n",
    "        if cur.parent == cur:\n",
    "            break\n",
    "        cur = cur.parent\n",
    "    return start.resolve()\n",
    "\n",
    "ROOT_DIR = find_project_root(Path.cwd())\n",
    "print(f\"üìÇ ROOT_DIR set to: {ROOT_DIR}\")\n",
    "print(f\"üìç CWD is: {Path.cwd()}\")\n",
    "\n",
    "# --- Paths (match STEP 5/6/8 layout) ---\n",
    "GOLD_PATH     = ROOT_DIR / \"data\" / \"inputs\" / \"inputs.xlsx\"\n",
    "GENERATED_DIR = ROOT_DIR / \"data\" / \"outputs1\" / \"md\"\n",
    "EVAL_DIR      = ROOT_DIR / \"data\" / \"outputs1\" / \"evaluation_results\"\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìó Gold Excel: {GOLD_PATH}\")\n",
    "print(f\"üìò Generated MD folder: {GENERATED_DIR}\")\n",
    "print(f\"üìô Evaluation folder: {EVAL_DIR}\")\n",
    "\n",
    "# --- Validate paths with helpful diagnostics ---\n",
    "if not GOLD_PATH.exists():\n",
    "    print(\"\\n‚ùå GOLD_PATH not found.\")\n",
    "    print(\"üîé Debug tips:\")\n",
    "    print(\" - Make sure your notebook is running inside the DMP-RAG project folder.\")\n",
    "    print(\" - Or update the root detection markers (README.md/config/) to match your repo.\")\n",
    "    print(f\" - Detected ROOT_DIR: {ROOT_DIR}\")\n",
    "    inputs_dir = ROOT_DIR / \"data\" / \"inputs\"\n",
    "    if inputs_dir.exists():\n",
    "        print(f\"‚úÖ Found data/inputs at: {inputs_dir}\")\n",
    "        print(\"üìÅ Contents of data/inputs/:\")\n",
    "        for p in sorted(inputs_dir.iterdir()):\n",
    "            print(\"   -\", p.name)\n",
    "    raise FileNotFoundError(f\"‚ùå Gold Excel not found: {GOLD_PATH}\")\n",
    "\n",
    "if not GENERATED_DIR.exists():\n",
    "    raise FileNotFoundError(f\"‚ùå Generated markdown folder not found: {GENERATED_DIR}\")\n",
    "\n",
    "# --- Load gold reference (Excel) ---\n",
    "df_gold = pd.read_excel(GOLD_PATH)\n",
    "df_gold.columns = df_gold.columns.str.strip().str.lower()\n",
    "df_gold = df_gold.fillna(\"\").astype(str)\n",
    "\n",
    "def normalize_title(name: str) -> str:\n",
    "    name = (name or \"\").lower()\n",
    "    name = re.sub(r\"[^a-z0-9\\s]\", \" \", name)\n",
    "    name = re.sub(r\"\\s+\", \" \", name).strip()\n",
    "    return name\n",
    "\n",
    "if \"title\" not in df_gold.columns:\n",
    "    raise ValueError(\"‚ùå Excel must contain a 'title' column (case-insensitive).\")\n",
    "\n",
    "df_gold[\"title_norm\"] = df_gold[\"title\"].apply(normalize_title)\n",
    "print(f\"‚úÖ Loaded {len(df_gold)} gold projects.\")\n",
    "\n",
    "# --- Detect gold element columns automatically ---\n",
    "element_cols = [c for c in df_gold.columns if c.startswith(\"element\")]\n",
    "if not element_cols:\n",
    "    raise ValueError(\"‚ùå No element columns found in Excel. Expected columns starting with 'element'.\")\n",
    "print(f\"üß© Detected {len(element_cols)} element columns: {element_cols}\")\n",
    "\n",
    "# --- Models ---\n",
    "print(\"üöÄ Loading evaluation models...\")\n",
    "sbert = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "rouge = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "print(\"‚úÖ Models ready.\")\n",
    "\n",
    "# --- Markdown parsing helpers ---\n",
    "def is_title(line: str) -> bool:\n",
    "    s = line.strip()\n",
    "    if s.startswith(\"#\"):\n",
    "        return True\n",
    "    if re.match(r\"^\\s*\\d+\\.?\\s*\\*\\*.*\\*\\*\\s*$\", s):\n",
    "        return True\n",
    "    if re.match(r\"^\\s*\\*\\*.*\\*\\*\\s*$\", s):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def strip_think_blocks(text: str) -> str:\n",
    "    return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL)\n",
    "\n",
    "def extract_sections(md_path: Path) -> pd.DataFrame:\n",
    "    text = md_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    text = strip_think_blocks(text)\n",
    "\n",
    "    lines = text.splitlines()\n",
    "    entries, current_title, buf = [], None, []\n",
    "\n",
    "    for ln in lines:\n",
    "        if is_title(ln):\n",
    "            if current_title and any(x.strip() for x in buf):\n",
    "                entries.append({\n",
    "                    \"Section Title\": current_title.strip(),\n",
    "                    \"Generated Content\": \"\\n\".join(buf).strip()\n",
    "                })\n",
    "            current_title, buf = ln, []\n",
    "        else:\n",
    "            buf.append(ln)\n",
    "\n",
    "    if current_title and any(x.strip() for x in buf):\n",
    "        entries.append({\n",
    "            \"Section Title\": current_title.strip(),\n",
    "            \"Generated Content\": \"\\n\".join(buf).strip()\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(entries)\n",
    "\n",
    "def clean_text_minimal(text: str) -> str:\n",
    "    text = strip_think_blocks(text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# --- Compare (exact title match on normalized) ---\n",
    "md_files = sorted(GENERATED_DIR.glob(\"*.md\"))\n",
    "print(f\"üîç Found {len(md_files)} generated Markdown files.\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for md_file in tqdm(md_files, desc=\"üìä Comparing element-level\"):\n",
    "    gen_title_norm = normalize_title(md_file.stem)\n",
    "\n",
    "    gold_row_df = df_gold[df_gold[\"title_norm\"] == gen_title_norm]\n",
    "    if gold_row_df.empty:\n",
    "        print(f\"‚ö†Ô∏è No gold match for file: {md_file.name}\")\n",
    "        continue\n",
    "\n",
    "    gold_row = gold_row_df.iloc[0]\n",
    "    gold_title = gold_row[\"title\"]\n",
    "\n",
    "    # Gold element texts (non-empty only)\n",
    "    gold_texts = {}\n",
    "    for c in element_cols:\n",
    "        val = str(gold_row.get(c, \"\")).strip()\n",
    "        if val:\n",
    "            gold_texts[c] = clean_text_minimal(val)\n",
    "\n",
    "    if not gold_texts:\n",
    "        print(f\"‚ö†Ô∏è Empty gold elements for: {gold_title}\")\n",
    "        continue\n",
    "\n",
    "    # Extract generated sections\n",
    "    gen_df = extract_sections(md_file)\n",
    "    if gen_df.empty:\n",
    "        print(f\"‚ö†Ô∏è No sections extracted from: {md_file.name}\")\n",
    "        continue\n",
    "\n",
    "    gen_df[\"Generated Content\"] = gen_df[\"Generated Content\"].astype(str).apply(clean_text_minimal)\n",
    "    gen_df = gen_df[gen_df[\"Generated Content\"].str.len() > 0].reset_index(drop=True)\n",
    "    if gen_df.empty:\n",
    "        print(f\"‚ö†Ô∏è All extracted sections empty after cleaning: {md_file.name}\")\n",
    "        continue\n",
    "\n",
    "    # --- SBERT caching for speed ---\n",
    "    gen_text_list = gen_df[\"Generated Content\"].tolist()\n",
    "    gen_emb = sbert.encode(gen_text_list, convert_to_tensor=True)\n",
    "\n",
    "    for element_name, gold_text in gold_texts.items():\n",
    "        emb_gold = sbert.encode(gold_text, convert_to_tensor=True)\n",
    "\n",
    "        sims = util.cos_sim(emb_gold, gen_emb)[0]\n",
    "        best_idx = int(sims.argmax().item())\n",
    "        best_sbert = float(sims[best_idx].item())\n",
    "\n",
    "        best_section_title = str(gen_df.loc[best_idx, \"Section Title\"]).strip()\n",
    "        best_gen_text = str(gen_df.loc[best_idx, \"Generated Content\"]).strip()\n",
    "        best_rouge = float(rouge.score(gold_text, best_gen_text)[\"rougeL\"].recall)\n",
    "\n",
    "        results.append({\n",
    "            \"Gold Project\": gold_title,\n",
    "            \"Gold Element\": element_name,\n",
    "            \"Generated File\": md_file.name,\n",
    "            \"Generated Section Title\": best_section_title,\n",
    "            \"SBERT_Similarity\": round(best_sbert, 4),\n",
    "            \"ROUGE_L_Recall\": round(best_rouge, 4),\n",
    "        })\n",
    "\n",
    "# --- Save ---\n",
    "df_results = pd.DataFrame(results)\n",
    "out_path = EVAL_DIR / \"element_similarity_exact_titles.csv\"\n",
    "df_results.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\n‚úÖ Element-level similarity saved to: {out_path}\")\n",
    "print(f\"üßæ Total element‚Äìsection best matches: {len(df_results)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b457323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ ROOT_DIR set to: C:\\Users\\Nahid\\dmpchef\n",
      "üìç CWD is: c:\\Users\\Nahid\\dmpchef\\notebook_DMP_RAG\n",
      "üìô EVAL_DIR: C:\\Users\\Nahid\\dmpchef\\data\\outputs1\\evaluation_results\n",
      "\n",
      "‚úÖ Loaded full-document (26 rows) from: full_dmp_pdf_comparison_fuzzy.csv\n",
      "‚úÖ Loaded element-level (312 rows) from: element_similarity_exact_titles.csv\n",
      "\n",
      "üìä Full-document summary table (Mean only, by Generated_File):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Generated_File</th>\n",
       "      <th>SBERT</th>\n",
       "      <th>ROUGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analysis of social media posts.md</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Basic Research from a Non-Human Source Example.md</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Clinical Data from Human Research Participants.md</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Clinical and MRI data from human research part...</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Clinical data (human biospecimens).md</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Clinical data from human research participants...</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Drug discovery including intellectual property.md</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gene expression analysis data from non-human m...</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Genomic data from a non-human source.md</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Genomic data from human research participants.md</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HeLa Cell Whole Genome Sequence (DNA or RNA).md</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Human Clinical Trial Data.md</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Human clinical and genomic data-NIA.md</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Human clinical and genomics data.md</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Human genomic data.md</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Human survey data.md</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Non-human data (primates).md</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Non-human data (rodents)-NIA.md</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Secondary Data Analysis Example.md</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Secondary Data Analysis on Data from Human Sub...</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Secondary data analysis-NIA.md</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Secondary data analysis.md</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Survey and Interview Example.md</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Survey and interview data-NIA.md</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Survey, interview, and biological data (tiered...</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Technology development.md</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Generated_File SBERT ROUGE\n",
       "0                   Analysis of social media posts.md  0.77  0.38\n",
       "1   Basic Research from a Non-Human Source Example.md  0.83  0.44\n",
       "2   Clinical Data from Human Research Participants.md  0.71  0.26\n",
       "3   Clinical and MRI data from human research part...  0.71  0.28\n",
       "4               Clinical data (human biospecimens).md  0.82  0.40\n",
       "5   Clinical data from human research participants...  0.76  0.31\n",
       "6   Drug discovery including intellectual property.md  0.80  0.37\n",
       "7   Gene expression analysis data from non-human m...  0.74  0.35\n",
       "8             Genomic data from a non-human source.md  0.71  0.33\n",
       "9    Genomic data from human research participants.md  0.72  0.28\n",
       "10    HeLa Cell Whole Genome Sequence (DNA or RNA).md  0.85  0.39\n",
       "11                       Human Clinical Trial Data.md  0.65  0.24\n",
       "12             Human clinical and genomic data-NIA.md  0.83  0.49\n",
       "13                Human clinical and genomics data.md  0.64  0.45\n",
       "14                              Human genomic data.md  0.68  0.36\n",
       "15                               Human survey data.md  0.73  0.32\n",
       "16                       Non-human data (primates).md  0.76  0.32\n",
       "17                    Non-human data (rodents)-NIA.md  0.77  0.60\n",
       "18                 Secondary Data Analysis Example.md  0.82  0.33\n",
       "19  Secondary Data Analysis on Data from Human Sub...  0.79  0.41\n",
       "20                     Secondary data analysis-NIA.md  0.73  0.31\n",
       "21                         Secondary data analysis.md  0.63  0.24\n",
       "22                    Survey and Interview Example.md  0.73  0.27\n",
       "23                   Survey and interview data-NIA.md  0.76  0.33\n",
       "24  Survey, interview, and biological data (tiered...  0.78  0.39\n",
       "25                          Technology development.md  0.74  0.58"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Element-level summary table (Mean ¬± SD):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Element</th>\n",
       "      <th>SBERT</th>\n",
       "      <th>ROUGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>element_1a</td>\n",
       "      <td>0.80 ¬± 0.14</td>\n",
       "      <td>0.48 ¬± 0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>element_1b</td>\n",
       "      <td>0.73 ¬± 0.11</td>\n",
       "      <td>0.43 ¬± 0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>element_1c</td>\n",
       "      <td>0.77 ¬± 0.09</td>\n",
       "      <td>0.49 ¬± 0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>element_2</td>\n",
       "      <td>0.80 ¬± 0.11</td>\n",
       "      <td>0.47 ¬± 0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>element_3</td>\n",
       "      <td>0.78 ¬± 0.13</td>\n",
       "      <td>0.46 ¬± 0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>element_4a</td>\n",
       "      <td>0.79 ¬± 0.12</td>\n",
       "      <td>0.55 ¬± 0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>element_4b</td>\n",
       "      <td>0.79 ¬± 0.10</td>\n",
       "      <td>0.49 ¬± 0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>element_4c</td>\n",
       "      <td>0.83 ¬± 0.08</td>\n",
       "      <td>0.51 ¬± 0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>element_5a</td>\n",
       "      <td>0.76 ¬± 0.13</td>\n",
       "      <td>0.46 ¬± 0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>element_5b</td>\n",
       "      <td>0.76 ¬± 0.09</td>\n",
       "      <td>0.41 ¬± 0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>element_5c</td>\n",
       "      <td>0.81 ¬± 0.15</td>\n",
       "      <td>0.49 ¬± 0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>element_6</td>\n",
       "      <td>0.85 ¬± 0.09</td>\n",
       "      <td>0.60 ¬± 0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Element        SBERT        ROUGE\n",
       "0   element_1a  0.80 ¬± 0.14  0.48 ¬± 0.29\n",
       "1   element_1b  0.73 ¬± 0.11  0.43 ¬± 0.25\n",
       "2   element_1c  0.77 ¬± 0.09  0.49 ¬± 0.28\n",
       "3    element_2  0.80 ¬± 0.11  0.47 ¬± 0.24\n",
       "4    element_3  0.78 ¬± 0.13  0.46 ¬± 0.27\n",
       "5   element_4a  0.79 ¬± 0.12  0.55 ¬± 0.23\n",
       "6   element_4b  0.79 ¬± 0.10  0.49 ¬± 0.24\n",
       "7   element_4c  0.83 ¬± 0.08  0.51 ¬± 0.23\n",
       "8   element_5a  0.76 ¬± 0.13  0.46 ¬± 0.26\n",
       "9   element_5b  0.76 ¬± 0.09  0.41 ¬± 0.20\n",
       "10  element_5c  0.81 ¬± 0.15  0.49 ¬± 0.32\n",
       "11   element_6  0.85 ¬± 0.09  0.60 ¬± 0.25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Saved formatted tables ‚Üí\n",
      "‚Ä¢ C:\\Users\\Nahid\\dmpchef\\data\\outputs1\\evaluation_results\\summary_full_table_mean_only.csv\n",
      "‚Ä¢ C:\\Users\\Nahid\\dmpchef\\data\\outputs1\\evaluation_results\\summary_element_table_mean_sd.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# üßÆ STEP 8 ‚Äî Summarize Evaluation Results (ROBUST ROOT DETECTION + outputs1/)\n",
    "# ============================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------------------\n",
    "# ‚úÖ Robust project root finder\n",
    "# ---------------------------\n",
    "def find_project_root(start: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Walk upward from `start` and return the first directory that looks like the project root.\n",
    "    Heuristics: contains data/ and (README.md or config/).\n",
    "    Also accepts folder name 'DMP-RAG' if present.\n",
    "    \"\"\"\n",
    "    cur = start.resolve()\n",
    "    for _ in range(25):\n",
    "        data_dir = cur / \"data\"\n",
    "        has_data = data_dir.exists() and data_dir.is_dir()\n",
    "        has_readme = (cur / \"README.md\").exists()\n",
    "        has_config = (cur / \"config\").exists() and (cur / \"config\").is_dir()\n",
    "\n",
    "        if has_data and (has_readme or has_config):\n",
    "            return cur\n",
    "        if cur.name.lower() == \"dmp-rag\":\n",
    "            return cur\n",
    "\n",
    "        if cur.parent == cur:\n",
    "            break\n",
    "        cur = cur.parent\n",
    "    return start.resolve()\n",
    "\n",
    "ROOT_DIR = find_project_root(Path.cwd())\n",
    "print(f\"üìÇ ROOT_DIR set to: {ROOT_DIR}\")\n",
    "print(f\"üìç CWD is: {Path.cwd()}\")\n",
    "\n",
    "# --- Evaluation directory (outputs1) ---\n",
    "EVAL_DIR = ROOT_DIR / \"data\" / \"outputs1\" / \"evaluation_results\"\n",
    "print(f\"üìô EVAL_DIR: {EVAL_DIR}\")\n",
    "\n",
    "# --- Input CSVs ---\n",
    "full_path = EVAL_DIR / \"full_dmp_pdf_comparison_fuzzy.csv\"\n",
    "elem_path = EVAL_DIR / \"element_similarity_exact_titles.csv\"\n",
    "\n",
    "# --- Diagnostics if missing ---\n",
    "missing = []\n",
    "if not full_path.exists():\n",
    "    missing.append(full_path.name)\n",
    "if not elem_path.exists():\n",
    "    missing.append(elem_path.name)\n",
    "\n",
    "if missing:\n",
    "    print(\"\\n‚ùå Missing required evaluation files:\", \", \".join(missing))\n",
    "    print(\"üîé Debug tips:\")\n",
    "    print(\" - Make sure you ran STEP 6 and STEP 7 successfully first.\")\n",
    "    print(f\" - Expected files under: {EVAL_DIR}\")\n",
    "\n",
    "    if EVAL_DIR.exists():\n",
    "        print(\"üìÅ Current contents of EVAL_DIR:\")\n",
    "        for p in sorted(EVAL_DIR.iterdir()):\n",
    "            print(\"   -\", p.name)\n",
    "    else:\n",
    "        print(\"‚ùå EVAL_DIR itself does not exist yet.\")\n",
    "\n",
    "    raise FileNotFoundError(f\"‚ùå Missing files in {EVAL_DIR}: {missing}\")\n",
    "\n",
    "# --- Load CSVs ---\n",
    "df_full = pd.read_csv(full_path)\n",
    "df_elem = pd.read_csv(elem_path)\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded full-document ({len(df_full)} rows) from: {full_path.name}\")\n",
    "print(f\"‚úÖ Loaded element-level ({len(df_elem)} rows) from: {elem_path.name}\\n\")\n",
    "\n",
    "# ============================================================\n",
    "# üß© 1Ô∏è‚É£ FULL-DOCUMENT LEVEL SUMMARY (Mean Only, by Generated_File)\n",
    "# ============================================================\n",
    "\n",
    "# Identify the project/file column\n",
    "if \"Generated_File\" in df_full.columns:\n",
    "    project_col = \"Generated_File\"\n",
    "else:\n",
    "    project_col = next(\n",
    "        (c for c in df_full.columns if \"generated\" in c.lower() and \"file\" in c.lower()),\n",
    "        None\n",
    "    ) or next(\n",
    "        (c for c in df_full.columns if \"file\" in c.lower() or \"title\" in c.lower() or \"project\" in c.lower()),\n",
    "        df_full.columns[0]\n",
    "    )\n",
    "\n",
    "# Identify numeric similarity columns\n",
    "sbert_col = next((c for c in df_full.columns if \"sbert\" in c.lower()), None)\n",
    "rouge_col = next((c for c in df_full.columns if \"rouge\" in c.lower()), None)\n",
    "\n",
    "if sbert_col is None or rouge_col is None:\n",
    "    raise ValueError(f\"‚ùå Could not find SBERT/ROUGE columns in df_full. Columns: {list(df_full.columns)}\")\n",
    "\n",
    "df_full_summary = (\n",
    "    df_full.groupby(project_col)[[sbert_col, rouge_col]]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_full_summary[\"SBERT\"] = df_full_summary[sbert_col].astype(float).apply(lambda x: f\"{x:.2f}\")\n",
    "df_full_summary[\"ROUGE\"] = df_full_summary[rouge_col].astype(float).apply(lambda x: f\"{x:.2f}\")\n",
    "\n",
    "df_full_table = df_full_summary[[project_col, \"SBERT\", \"ROUGE\"]].rename(\n",
    "    columns={project_col: \"Generated_File\"}\n",
    ")\n",
    "\n",
    "print(\"üìä Full-document summary table (Mean only, by Generated_File):\")\n",
    "display(df_full_table)\n",
    "\n",
    "# ============================================================\n",
    "# üß© 2Ô∏è‚É£ ELEMENT-LEVEL SUMMARY (Mean ¬± SD)\n",
    "# ============================================================\n",
    "\n",
    "elem_col = (\n",
    "    \"Gold Element\" if \"Gold Element\" in df_elem.columns\n",
    "    else next((c for c in df_elem.columns if \"element\" in c.lower()), df_elem.columns[0])\n",
    ")\n",
    "\n",
    "sbert_col_e = next((c for c in df_elem.columns if \"sbert\" in c.lower()), None)\n",
    "rouge_col_e = next((c for c in df_elem.columns if \"rouge\" in c.lower()), None)\n",
    "\n",
    "if sbert_col_e is None or rouge_col_e is None:\n",
    "    raise ValueError(f\"‚ùå Could not find SBERT/ROUGE columns in df_elem. Columns: {list(df_elem.columns)}\")\n",
    "\n",
    "df_elem_summary = (\n",
    "    df_elem.groupby(elem_col)[[sbert_col_e, rouge_col_e]]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Flatten columns\n",
    "df_elem_summary.columns = [elem_col, \"SBERT_Mean\", \"SBERT_SD\", \"ROUGE_Mean\", \"ROUGE_SD\"]\n",
    "\n",
    "df_elem_summary[\"SBERT\"] = df_elem_summary.apply(\n",
    "    lambda r: f\"{r['SBERT_Mean']:.2f} ¬± {r['SBERT_SD']:.2f}\", axis=1\n",
    ")\n",
    "df_elem_summary[\"ROUGE\"] = df_elem_summary.apply(\n",
    "    lambda r: f\"{r['ROUGE_Mean']:.2f} ¬± {r['ROUGE_SD']:.2f}\", axis=1\n",
    ")\n",
    "\n",
    "df_elem_table = df_elem_summary[[elem_col, \"SBERT\", \"ROUGE\"]].rename(columns={elem_col: \"Element\"})\n",
    "\n",
    "print(\"\\nüìä Element-level summary table (Mean ¬± SD):\")\n",
    "display(df_elem_table)\n",
    "\n",
    "# ============================================================\n",
    "# üíæ Save formatted tables\n",
    "# ============================================================\n",
    "out_full = EVAL_DIR / \"summary_full_table_mean_only.csv\"\n",
    "out_elem = EVAL_DIR / \"summary_element_table_mean_sd.csv\"\n",
    "\n",
    "df_full_table.to_csv(out_full, index=False)\n",
    "df_elem_table.to_csv(out_elem, index=False)\n",
    "\n",
    "print(f\"\\nüíæ Saved formatted tables ‚Üí\\n‚Ä¢ {out_full}\\n‚Ä¢ {out_elem}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

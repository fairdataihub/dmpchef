{
  "title": "NIH Data Management and Sharing Plan: Multimodal Imaging + EHR-Derived Clinical Outcomes Study",
  "funding_agency": "NIH",
  "use_rag": true,
  "inputs": {
    "research_context": "This project evaluates whether integrating multimodal clinical imaging with structured EHR-derived variables improves prediction of predefined clinical outcomes (e.g., diagnosis, progression, treatment response, adverse events, utilization, or survival endpoints defined in the protocol). The study includes cohort identification and phenotyping, data harmonization across sites/systems, quality assessment, and development/validation of predictive models using prespecified evaluation metrics. The work will generate reproducible analytic outputs, including curated datasets, feature representations, model artifacts, and documentation of all preprocessing and modeling decisions. Primary goals are to (1) create analysis-ready datasets that can be reused for secondary analyses, (2) produce model-ready features and labels that support replication, and (3) document a transparent pipeline that enables re-running the full workflow with versioned code and configuration.",
    "data_types": "Data types include: (1) clinical imaging data preserved in original DICOM format (with imaging-derived measurements and derived research formats used in analysis workflows, if applicable); (2) structured participant-level tabular data extracted from the EHR/EDW (e.g., demographics, encounter metadata, diagnoses, procedures, medications, laboratory values, vital signs, outcomes, and limited time variables that may be shifted/binned where necessary for privacy); (3) derived analytic datasets such as cleaned/normalized tables, harmonized code sets, feature matrices, embeddings, labels, and dataset partitions (train/validation/test splits with documented random seeds and cohort inclusion/exclusion rules); (4) model artifacts including trained weights/checkpoints, hyperparameter configurations, model cards or summary reports, inference scripts, and evaluation outputs (AUROC/PR curves, calibration summaries, subgroup analyses when permissible); (5) software/code including ETL scripts, preprocessing pipelines, modeling code, and environment specifications (requirements files, container definitions if used); and (6) supporting documentation such as data dictionaries, README files, protocol references, analytic workflow descriptions, and provenance notes capturing data lineage and transformation steps.",
    "data_source": "Data will be obtained from human participants under approved IRB protocol(s) and/or institutional clinical data governance approvals. Imaging originates from routine clinical care imaging systems (e.g., PACS) and/or research imaging acquisitions, with modality and acquisition parameters recorded as available. Structured variables will be extracted from institutional EHR/EDW sources and study case report forms (if applicable), using documented SQL queries and ETL scripts. Data curation will include standardization of variable definitions, mapping of clinical codes (e.g., ICD/CPT/LOINC/RxNorm when available), unit harmonization for labs, handling of missingness, and validation checks (range checks, duplicate detection, temporal consistency checks). The pipeline will maintain versioned extracts and curated outputs, with a change log describing updates to cohort definitions, feature engineering, or outcome definitions across iterations.",
    "human_subjects": "Yes. The project uses human participant data. Prior to sharing, datasets will be de-identified to the extent possible and reviewed to minimize re-identification risk. De-identification steps include removal of direct identifiers; review and minimization of quasi-identifiers; shifting, binning, or coarse-coding time fields (e.g., relative times instead of dates) when needed; and applying modality-appropriate de-identification for imaging (e.g., removal of PHI from DICOM headers and review of burned-in annotations when present). The team will follow institutional privacy and security policies and applicable regulations, and will perform a risk-based assessment to determine whether data can be shared openly, must be shared under controlled access, or must be shared only in aggregate form. Any limitations on sharing will be documented with rationale and the specific controls used.",
    "consent_status": "Participants provide consent consistent with study aims, including sharing of de-identified data when feasible. Consent language and any relevant data use agreements (DUAs) will be reviewed to determine allowable sharing scope, permitted repositories, and restrictions on secondary use. If consent/DUAs or institutional policy restrict sharing of certain data elements (e.g., small subgroups, sensitive conditions, high-resolution time fields, or image types with elevated re-identification risk), those components will be shared via controlled-access mechanisms. Controlled access will include data use agreements, review by an access committee or equivalent governance body, and user authentication/authorization requirements. The plan will document which data are shareable, which require restrictions, and the process for requesting and approving access, including expected timelines and criteria for approval.",
    "data_volume": "Estimated total volume is ~200â€“300 GB, depending on cohort size, imaging modalities, and the number of derived products retained. Imaging will be preserved in DICOM as the authoritative source format, with derived research-friendly representations shared when appropriate and permitted (e.g., standardized derived images or imaging-derived features). Tabular datasets will be stored and shared as CSV and/or Parquet to support interoperability and efficient reuse. Derived artifacts (feature matrices, embeddings, and labels) will be shared in documented formats with clear schemas and version identifiers. Documentation (README, data dictionaries, protocols, and workflow descriptions) will be provided in PDF/Markdown and maintained alongside code, with file-level versioning and release notes. Storage planning accounts for intermediate processing outputs, final curated releases, and long-term archiving of the minimum necessary set to reproduce key published results."
  }
}
